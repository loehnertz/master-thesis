\documentclass[12pt,a4paper]{report}

\include{settings}

\newcommand{\studyprogramme}{Software Engineering}
\newcommand{\degreetype}{Master of Science}
\newcommand{\thesistitle}{
Toward systematic decomposition of monolithic software into microservices
}
\newcommand{\thesissubtitle}{
SUBTITLE
}
\newcommand{\thesisauthor}{Jakob L{\"o}hnertz}
\newcommand{\thesisdate}{31/08/2019}
\newcommand{\thesislocation}{Amsterdam}
\newcommand{\firstmarker}{Dr.\ Ana Oprescu}
\newcommand{\secondmarker}{MSc.\ Stephan Schroevers}
\newcommand{\hostorganization}{Picnic B.V.}

\begin{document}




\include{preamble}




\begin{abstract}

ABSTRACT

\end{abstract}




\tableofcontents




\chapter{Introduction} \label{chap:introduction}

Software engineering develops faster than ever, the amount of novelties in
platforms, languages, operational strategies and generally the terminology
around it gets larger and larger every day.
One of the terms that gained a lot of traction in recent years is \textit{microservices}.
Many engineers talk about their benefits and challenges but the general notion
is positive and the buzz is undeniable.
At the end of the day, microservices are nothing more than an architectural style
for designing back-end software.
For those that actually take the step to build and deploy such an application
landscape, the journey is not always straightforward.
The idea is still evolving and mandates many new strategies and concepts
that developers and operations have to deal with
when choosing to devote their business logic to leverage microservices.

There are many good reasons to employ this pattern nowadays specifically
when dealing with ever larger growing monolithic applications that try to
encapsulate their respective business logic without drowning the
developers and engineers working on it in seemingly endless complexity
that a single human cannot even fully comprehend anymore at one point.

For many companies, this oftentimes is the status quo and if the decision is
made to move to a microservices\hyp based architecture, one of the first
challenges, the decomposition, is already difficult to begin with:

Defining where within the existing application one service should end and the
next one should begin --- i.e. detecting latent boundaries that partition
the monolith into many smaller pieces, is a very manual and tedious task.

Thus, this thesis develops an algorithmic methodology that can assist
a human software engineer in performing the aforementioned decomposition.



\section{Problem analysis}

Generally speaking, there are two ways to put a microservices\hyp based
software architecture into place. The greenfield approach, meaning that
microservices are directly getting built without a preexisting application,
as well as the transformation of an existing monolithic software solution
into independent microservices \cite{fowler-break-monolith}.
However, this thesis focuses solely on the latter approach for a variety of reasons.

Firstly, our work leverages static and dynamic program analysis for solving
the posed task. Obviously, this is not possible without an already running solution.

Secondly, while there is already some research on this topic \cite{fritzsch2018monolith},
which is discussed in the \textit{\nameref{chap:related}} chapter, the notion
of utilizing the already existing and running application is still underrepresented.

Thirdly, as Martin Fowler remarks, microservices are especially feasible
with a mature workflow, domain model, operational model, etc. already
in place \cite{fowler-monolith-first, fowler-microservices-tradeoffs}
which favors non\hyp greenfield approaches as well.

The task of getting the boundaries of each microservice right is hard,
especially when the teams working on the existing software
are caught up in their mental models of it \cite{latoza2006maintaining},
as Fritzsch et al. confirm:
\begin{displayquote}
\emph{"Extracting a domain model from an application's code base
can be a significant challenge. If incorrectly applied,
it can lead to architectures that combine the drawbacks of both styles,
monolithic architectures and Microservices."}~\cite{fritzsch2018monolith}
\end{displayquote}
To prevent such an outcome, we propose a semi-automated approach to assist
software engineers in decomposing microservices out of an existing monolithic code base.
This approach has the advantage that the mental models regarding the existing
solution might be broken apart partially to offer new perspectives onto how
the current software works, which was already remarked as a positive side effect
of an assisted microservices decomposition approach \cite{gysel2016service}.
Thus, the engineers and architects can be assisted in the first step,
which is likely also the most difficult and time-consuming one due to the possible
complexity of the system \cite{fritzsch2018monolith, france2007model},
when undertaking the operation of transforming their existing
monolithic solution into microservices.

Additionally, we are certain that the actual decisions should still be made
by seasoned engineers and architects which is why a semi-automated solution
was determined as the best option to support them in their
decision making while not taking the decision making away from them
(i.e. compared to an automated decomposition approach).
As a consequence, everything mentioned in this section reflects itself in
the following research questions of this thesis.



\section{Research questions}

\hangindent=1cm
\noindent Thus, we derived the following research questions:\\
\textbf{RQ1:} \textit{"How can existing monolithic software be analyzed
and used to give microservices recommendations based on it?"}\\
\textbf{RQ2:} \textit{"Which numeric metrics are the most suitable
for evaluating microservice decomposition methodologies?"}



\section{Research methods} % TODO: Validate this section

Firstly, the approach for the methodology is developed and validated
for feasibility especially regarding its implementation.
Apart from its general description in the \textit{\nameref{chap:related}} chapter,
the approach itself is explained and devised in chapters \ref{} to \ref{}.

Secondly, the foundation for the rationale behind the methodology is worked out
as the approach itself is abstracted away from the topic of
software decomposition and instead built upon graph theory.
Precisely, this encompasses a mapping of the inputs into the methodology to
their graph theoretic representation as well as the final output.
Otherwise, no logic reasoning is possible to tie the results and
their calculated metrics back to the inputs and the underlying subject
of decomposing monolithic software into microservices in the first place.
This part of the research is covered in chapter \ref{chap:rationale}.

Likewise, not only a theoretical methodology but also an implemented
proof-of-concept (PoC) are the outputs of this project.
The latter is introduced in chapter \ref{}.
The steps that this PoC needs to conduct are derived and tested
independently via exploratory as well as confirmatory case studies
in chapter \ref{} to receive substantive, numeric,
and separate results \cite{easterbrook2008selecting}.
Moreover, controlled experiments are not feasible as control results
with their dependent variables for the outcomes are rare and difficult
to find and measure \cite{easterbrook2008selecting}.

Finally, the research is validated with real-world monolithic applications.
A large-scale code base provided by the company hosting this thesis as well as
large open source software projects were candidates for this.
The evaluation as part of chapter \ref{} is conducted using
a custom set of metrics devised in chapter \ref{}.




\chapter{Background} \label{chap:background}

Three domains are mainly important for grasping the entirety of this work.
General computer science knowledge is expected but the following
three subjects are covered in more detail on top of it:
\textit{\nameref{sect:background-architecture}},
\textit{\nameref{sect:background-program-analysis}}, and
\textit{\nameref{sect:background-graph-clustering}}.



\section{Software architectures} \label{sect:background-architecture}

As seen in the \textit{\nameref{chap:introduction}}, our work revolves around
the decomposition of monolithic software into microservices.
Thus, the following subsections focus on these two architectural styles for
building back-end applications.


\subsection{Monolithic}

The term \textit{monolithic} is derived from the Ancient Greek word
\textit{monólithos} which roughly translates to \textit{single stone} \cite{press2011oxford}.
This adumbrates already what the word is used for in software engineering,
where it, specifically regarding enterprise back-end applications,
describes software that is built, deployed, and executed
as a single logical unit \cite{ms-fowler}.
Furthermore, a monolithic application can perform every task of the given functionality
within the domain of its business logic from beginning to end \cite{monolith}.
Usually this is implemented in a layered (e.g. UI, business logic, data access)
and modular manner \cite{ms-fowler, monolith}.


\subsubsection{Advantages}
Generally, the notion exists that monolithic applications are more natural
to work with while also being easier to design, build, and deploy specifically
in the beginning of their life cycle \cite{ms-fowler, raymond2003unix}.

Tying into this, the operational overhead is small as not much middleware or
special deployment strategies are necessary \cite{ms-fowler}.
This becomes specifically apparent when compared to microservices
in the following subsection.

Finally, the performance is usually better compared to architectures dependent
on inter-process communication \cite{knoche2016sustaining}.


\subsubsection{Disadvantages}
Even if the monolithic application is built modular and in layers,
the code base and most likely the complexity can just grow over time.
The engineer is in the end just fighting against it becoming a
\textit{big ball of mud} which starts to exceed the mental capabilities of a
human engineer \cite{newman2015building, foote1997bigballofmud, france2007model}.

Moreover, a monolithic application is deployed as a whole which hinders
independent life cycles regarding the modules within the application \cite{ms-fowler}.
Specifically in agile software development structures, this can cause immense
conflicts when trying to release new versions of the application.

Finally, following the last disadvantage, the scalability is also problematic
as the only option once the application needs to be scaled is to replicate it
into another instance and balance incoming load onto the instances ---
even if only a tiny module within the monolith might have needed any scaling
in the first place \cite{ms-fowler, newman2015building}.


\subsection{Microservices\hyp based}

Microservices are essentially about building an application
\textit{"as a suite of small services, each running in their own process and
communicating with lightweight mechanisms, often an HTTP resource API"}
\cite{ms-fowler} --- a microservice is developed and deployed independently
in a distributed manner.
Each service should be built around one business capability following the
\textit{single responsibility principle} \cite{newman2015building, martin2003agile}.
Furthermore, each microservice should be responsible for the data it governs,
avoiding any overlaps with other services \cite{ms-fowler}.


\subsubsection{Advantages}
As just mentioned, microservices\hyp based architectures allow for a better
decoupling of software development life cycles between business capabilities \cite{ms-fowler}.
This enables different teams or even single developers to work at different
velocities which is sometimes also dictated by the type and complexity of the
developed functionality.

Additionally, once the foundation of the architectural design is established,
it is a lot more effortless to keep functionalities decoupled as
they are not even part of the same process once running.

Moreover, it is easier to incorporate multiple programming languages into
the application landscape that fit the tasks the best.
In the end, each service just offers an external API for the others to use
and consume which is agnostic of programming language as long as it adheres
to the contract provided by the interface specification.

Finally, the scalability of microservices is far better compared to monolithic
applications. When leveraging features of cloud computing, just services
that are in need of scaling can be scaled instead of the entirety of the application.


\subsubsection{Disadvantages}
In contrast to monolithic applications, microservices introduce a non\hyp negligible
amount of overhead. Firstly, the architectural style is geared toward
agile software development which by itself mandates a certain organizational setup.
Secondly, the development demands other patterns in terms of e.g.
error-handling and data management compared to a monolithic software design.
Lastly, the deployment is more fine-grained and usually encompasses a lot of
cloud computing facilities such as \textit{Infrastructure as a Service} (IaaS)
together with certain topics completely unheard of in monolithic architectures
such as \textit{service discovery} which is dealing with the task of each
service knowing where and how to reach the others \cite{newman2015building}.

Furthermore, the penalty on inter-service communication via a network is almost
always larger compared to regular function calls, especially when considering
serialization and deserialization before and after every inter-service call
\cite{knoche2016sustaining, newman2015building}.

Additionally, designing microservices, similarly to a modular monolithic design,
is a grand task. However, the boundaries need to be defined even more strictly
as migrating certain functionality and data later on has a bigger impact than
doing the same in a monolithic application, since data can oftentimes
stay where it is in a monolithic architecture.
Likewise, the interfaces between services need more care than interfaces in a
monolithic application due to their distributed nature that can lead to
performance and fail-over problems a lot faster as a network is acting
in between.
Finally, Conway's law \cite{conway1968law} is more important than ever
in a microservices landscape as overly dependent life cycles between services
will lead to nothing more than a \textit{distributed monolith} in the long run.
\newline


In summary, the larger and more complicated the underlying business logic
becomes, the more the application can benefit from a microservices\hyp based architecture.
Specifically when developing the overarching application in agile teams,
microservices offer natural boundaries for this organizational structure.
Furthermore, when looking at the disadvantages of them, the question of where
to draw the boundaries between microservices is especially difficult,
yet it is what this thesis revolves around as it can benefit from
algorithmic assistance to the work of human software engineers a lot.



\section{Software analysis} \label{sect:background-program-analysis}

As no further user input should be required by our methodology,
it has to gather information out of inputs on its own.
Thus, various types of program analysis are leveraged to
algorithmically collect data that can be used as the input for the methodology.


\subsection{Static program analysis}

The term \textit{static program analysis} is best defined by comparing it
to \textit{dynamic program analysis} --- instead of monitoring the program
as it is running, the code, in many possible facets, is analyzed without
actually executing it. While static program analysis has many categories
\cite{woegerer2005static}, the only two sub-categories relevant to this work
are \textit{call graphs} and \textit{semantic analysis} while the
latter has its roots in \textit{natural language processing} especially in
the way that it is utilized in our methodology.


\subsubsection{Call graph generation} \label{subsubsect:call-graph-static}
Generally, a call graph is consisting of \textit{"nodes that are the routines
of the program and directed arcs that represent calls from call sites to
routines"} \cite{graham1982gprof}. Those graphs can be created via static
as well as dynamic program analysis, although the latter is covered in
subsection \ref{subsect:background-dynamic-analysis}.
In an object-oriented program, the routines would represent methods
or even whole classes \cite{grove1997callgraph}.

The two basic ways to represent the weight \(W(e_i) \in \mathbb{R}\)
of a weighted edge \(e_i \in E\) as part of a call graph \(G = (V, E)\)
are \textit{execution counts} and \textit{execution times} although
\textit{execution times} are only measurable via
dynamic program analysis \cite{graham1982gprof}.

However, in our approach, the static program analysis that we employ has
auxiliary purposes since other types of program analysis, particularly
dynamic program analysis, will seldom yield a complete graph theoretic
representation of the analyzed program \cite{graham1982gprof}, such that
each routine from the input program \(r_i \in R\) is part of the output graph
\(G = (V, E)\) as a vertex \(\forall r_i \in V\) while each \textit{execution}
\(x_i \in X\) is an edge \(\forall x_i \in E\) connecting
two such vertices \(x_i = (r_1, r_2), \enskip r_1 \in V, \enskip r_2 \in V\).

We utilize the static program analysis without \textit{execution counts}
which does not yield a weighted graph, allows us however to have a more complete
weighted call graph when merging two or more incomplete call graphs,
weighted or not, e.g. when another was constructed with dynamic program analysis.
Precisely, this is discussed in the chapter \textit{\nameref{chap:extracting-coupling}}.


\subsection{Dynamic program analysis} \label{subsect:background-dynamic-analysis}

Next to \textit{static program analysis}, one can also analyze the behavior of
an application while it is running, this is called
\textit{dynamic program analysis} \cite{ernst2003static}.
The reasons to utilize \textit{dynamic program analysis} are manifold,
to gather information about runtime performance, memory usage,
or debugging \cite{ernst2003static}.
Another application is the creation of a call graph as discussed in subsection
\ref{subsubsect:call-graph-static}. For the same reasons and a few others on top,
this the process of generating a \textit{dynamic call graph} are interested to our work.


\subsubsection{Call graph generation} \label{subsubsect:call-graph-dynamic}
Additionally to the aforementioned reasons to create a \textit{static call graph},
a \textit{dynamic call graph} can provide other useful information.
Precisely, to discover where the most data flows within the monitored application
is a main application of this process \cite{graham1982gprof}.
The common approach is to build a weighted and optionally directed graph
to represent the call graph. The optional directions as the directions the
routines invoke each other and e.g. \textit{execution counts} or average
\textit{execution times} as the weighted edges \cite{graham1982gprof}.
Specifically the latter can only be recorded during the runtime of the application,
however for our work only \textit{execution counts} are used as the edge weights.
The general problem with \textit{dynamic program analysis} however is that one
firstly has to run the application that is to be analyzed and secondly the fact
that there is generally no guarantee that even every part of the application
has been invoked even when analyzed for an immense amount of time which will
then lead to an incomplete \textit{dynamic call graph} \cite{graham1982gprof}.


\subsection{Semantic similarity calculation} \label{subsect:semantic-similarity}

Calculating the semantic similarity between two documents is a traditional
discipline in the domain of natural language processing (NLP).
The similarity value usually ranges from \([0.0, 1.0]\) ---
a percentage \cite{singhal2001ir}.

As anthropogenic source code is largely consisting of natural language
e.g. in the form of identifiers, variable names, and comments, it can be
semantically analyzed as well.

The basic steps to calculate semantic similarity between documents in
any given corpora (a \textit{corpus} is a collection of \textit{documents})
as a mathematical process are \cite{singhal2001ir}:
\begin{enumerate}
  \item Tokenization
  \item Stop word filtering
  \item Normalization
  \item Vectorization
  \item (Optional) Relevance weighting
  \item (Optional) Latent semantic indexing (LSI)
\end{enumerate}


\subsubsection{Similarity calculation}
All of the abovementioned steps have to be performed for every document
in the corpus until the resulting vectors can be run through a function that
calculates their similarity. A common approach for this is the
\textit{cosine similarity} which calculates the cosine of an angle \(\theta\)
between two input document vectors that are part of the corpus
\(\{\vec{\mathbf d_1}, \vec{\mathbf d_2}\} \in C\) \cite{singhal2001ir}:
\[
  \cos(\theta) = \frac{
    \vec{\mathbf d_1} \cdot \vec{\mathbf d_2}
  }{
    \|\vec{\mathbf d_1} \|\|\vec{\mathbf d_2} \|
  }
\]


\subsubsection{Tokenization}
The goal of the first step is to retrieve a list of terms (i.e. words)
of a given document in the corpus \(\forall t_i \in D_i, \enskip D_i \in C\).
Importantly, each document \(D_i\) is a sequence not a set
(i.e. is allows the repetition of elements), otherwise the 5th step would not
work as shown later.
Consequently, punctuation characters are stripped from the document and
the text get split for instance on spaces \cite{singhal2001ir}.


\subsubsection{Stop word filtering}
Language naturally contains terms that do not carry much information,
those are called \textit{stop words} \cite{singhal2001ir}.
Common examples from the English language are words such as:
\textit{the}, \textit{and}, \textit{or} etc.
The useful semantics can be found in the words that are left after filtering
out such \textit{stop words}.


\subsubsection{Normalization}
Both of these terms refer to the process of removing the endings of all terms
in a given document \(\{t_i \in D_i \mid normalization(t_i)\}\) that is part of
the corpus \(D_i \in C\) \cite{singhal2001ir}.

The rationale of this step is to normalize the semantics of the terms ---
\(\{\textit{organization}, \textit{organizer}, \textit{organizing}\}\) essentially
encapsulate the same semantic meaning of their \textit{lemma}: \textit{organize}.
Nevertheless, if not normalized, a purely mathematical approach,
which is utilized here, will not capture the common denominator of those terms.
Consequently, each term is stripped of its ending. There are two major approaches
to do so: \textit{lemmatization} and \textit{stemming} \cite{plisson2004lemma}.

The \textit{lemma} of a word is its \textit{canonical form}, its base form \cite{plisson2004lemma}.
Thus, \textit{lemmatization} describes the process of normalizing words to their
\textit{lemmata} \cite{plisson2004lemma}.

The other variant, the \textit{stemming}, describes a similar process with the
same goal that instead just strips common endings in the respective language.
Therefore, \textit{lemmatization} is objectively the better, albeit more complex,
approach as it captures the actual base form of a word while also
being less error-prone. A good example are the English words:
\(\{\textit{runs}, \textit{running}, \textit{ran}\}\). It becomes apparent that
in this example, the \textit{stemming} approach would not have correctly
normalized the word \textit{ran}, compared to the \textit{lemmatization} approach.

Ergo, if possible, \textit{lemmatization} is preferred over \textit{stemming}.


\subsubsection{Vectorization}
Since the described process is mathematical in its core, the sequence of terms
out of each document of the corpus needs to be converted to
numerical representations of themselves.
Precisely, this step involves constructing a term-document matrix
with the all terms contained in the corpus \(\forall t_i \in \bigcup C\)
as the rows and all the documents in the corpus \(\forall D_i \in C\)
as the columns. It is filled with either a \(0\) if a term of a given row
does not appear in the document of that column or a \(1\) if it does appear:
\[
  \begin{matrix}
    & \textbf d_i \\
    & \downarrow \\
    \textbf t_i^{T} \rightarrow &
    \begin{bmatrix}
      td_{1,1}  & \dots   & 1       & \dots   & td_{1,n}  \\
      \vdots    & \ddots  & \vdots  & \ddots  & \vdots    \\
      1         & \dots   & 0       & \dots   & 0         \\
      \vdots    & \ddots  & \vdots  & \ddots  & \vdots    \\
      td_{m,1}  & \dots   & 0       & \dots   & td_{m,n}  \\
    \end{bmatrix}
  \end{matrix}
\]
Consequently, this results in a document vector
\(\vec{\mathbf d_i}\) for every column \(td_{*,n}\)
of the matrix, which is necessary for the remaining steps.


\subsubsection{Relevance weighting}
Although the resulting document vectors could already be used for the
\textit{cosine similarity} calculation, the quality of the output
can still be improved by weighting every term to increase or decrease its value
in the matrix per document based on its relevance for that document in relation
to the whole corpus.

A common approach, that we are utilizing,
is \textit{term frequency–inverse document frequency} (\textit{tf-idf}).
The idea behind it is to increase the weight of terms that have
a high \textit{term frequency} and a low \textit{inverse document frequency}.
This tends to filter out common terms while overall improving the relevance
of the terms in the corpus \cite{robertson2004tfidf, singhal2001ir}.
Theoretically, one can skip the step of \textit{stop word filtering}
as \textit{tf-idf} in the end achieves a similar goal by not returning
high values for common terms that might have been considered \textit{stop words}.

It first independently calculates the \textit{term frequency} for each term,
such that the frequency of a term in a given document is divided by the total
amount of terms in that document \cite{robertson2004tfidf}.
The goal is to normalize the importance of a term owing to the fact
that longer documents have a higher chance of containing the given term
more often \cite{singhal2001ir}:
\[tf(t,d) = \frac{f_{t, d}}{\vert d \vert}\]

Next, the \textit{inverse document frequency} is calculated for each term,
such that the logarithm of the total amount of documents in the corpus
is divided by the amount of documents containing the term \cite{robertson2004tfidf}.
This calculation quantifies the importance of a term toward the corpus \cite{singhal2001ir}:
\[idf(t,C) = \log \left(\frac{\vert C \vert}{\vert \{d \in C \mid t \in d\} \vert}\right)\]

Finally, the functions are combined to calculate the \textit{tf-idf} value
of each term on a per-document basis \cite{robertson2004tfidf}:
\[tfidf(t,d,C) = tf(t,d) \times idf(t,C)\]

Applying \textit{tf-idf} changes the values of the elements in the matrix
from \(\forall td_{m,n} \in \{td \in \mathbb{Z} \mid td \le 0\}\)
to \(\forall td_{m,n} \in \{td \in \mathbb{R} \mid td \le 0\}\) due to
the two divisions involved.


\subsubsection{Latent semantic indexing (LSI)}
This last step is optional but oftentimes utilized to further improve the quality
of the output of this process. \textit{Latent semantic indexing} (LSI) describes
the process of applying \textit{singular value decomposition} (SVD) to
the aforementioned term-document matrix \cite{deerwester1990lsi}. % TODO: Should one go into more detail?
The rationale in the context of the similarity calculation is
to reduce the matrix with \(m\) rows and \(n\) columns to one with a
set amount of \(z\) dimensions as the rows.
This is supposed to reduce the effects of \textit{synonymy} and
\textit{polysemy} \cite{deerwester1990lsi}.
Precisely, \textit{synonymy} describes the phenomenon where different terms describe
the same idea \cite{press2011oxford} whereas \textit{polysemy} describes the phenomenon
where same term describes multiple ideas \cite{press2011oxford}.
After applying \textit{LSI}, each document vector of the corpus \(\vec{\mathbf d_i} \in C\)
has a magnitude of the previously chosen \(z\) amount of dimensions \(\|\vec{\mathbf d_i}\|\).
Consequently, the abovementioned \textit{cosine similarity} can be calculated.


\subsubsection{Source code processing}
The main difference when processing source code occurs in the first step,
the \textit{tokenization}.
Truly natural language does not use many special characters such as
braces, semicolons, or ampersands which conversely see heavy usage in
many programming languages.
As a consequence, the \textit{tokenization} is specific to the respective
programming language that is getting processed as different languages use a
different set of control characters together with differing strategies
for concatenating identifiers and variable names consisting of
multiple words (e.g. camel case or snake case).

Furthermore, the \textit{stop word filtering}, if employed, has to be tailored
even more to the programming language getting processed ---
keywords (e.g. \code{public}, \code{static}, \code{void}) appear in
every source code document while not adding any meaningful semantic information,
hence it is useful to filter those out as part of the \textit{stop word filtering}.


\subsection{Evolutionary similarity calculation} \label{subsect:evolutionary-similarity}

Similarly to the \textit{semantic similarity}, it can be measured how much the
software development life cycles of two entities (e.g. classes) do overlap.
Precisely, every entity in an application has a life cycle regarding
its creation, modifications (i.e. maintenance), and eventual deletion.
For instance, entities that fulfill a similar functionality or have some
interfaces toward each other have a high likelihood of having
\textit{evolutionary similarity} owing to the fact that the functionality
they encompass is spread over multiple entities and maintenance oftentimes
incorporates changes in multiple ones as well.

The goal again is to get a percentage that resembles how similar the life cycles
of two given entities are, in the same vein to the \textit{semantic similarity}.
Compared to the previously exhibited \textit{semantic similarity} however,
the \textit{evolutionary similarity} has the downside of not having a renowned
or standard approach to retrieve the abovementioned percentage.
We chose a generally applicable approach based on the work by Tornhill
on \textit{evolutionary similarity} \cite{tornhill2015crimescene}.

Essentially, this approach is based on mining information out of
version control system (VCS) data such as a Git log.
The listing \ref{git-log} depicts this with one example revision entry
out of a large Git log of a real application. However, multiple of those
revision entries would be analyzed to calculate \textit{evolutionary similarity}.

Precisely, over the duration of a certain time period \([t_1, t_2]\)
or the entirety of the log \([t_{min}, t_{max}]\), every revision \(r \in R\)
contains at least one entity (i.e. file) out of the code base at that point
in time \(\exists e \in r, e \in E\) that was modified
(including creation and deletion) as part of the revision.
Over the course of all the revisions \(\forall r \in R\) in a given time period
\([t_1, t_2]\), the revisions where an entity \(e_1 \in E\) changed together
with another entity \(e_2 \in E\) are gathered:
\[
  R' = \{ r' \in \{ r \in R \mid r \in [t_1, t_2] \} \mid e_1 \in r' \land e_2 \in r' \}
\]
Furthermore, the ratio of how many of the total revisions that modified
the entity \(e_1\) is calculated to obtain a percentage:
\[
  \frac{\vert R' \vert}{\{ r' \in \{ r \in R \mid r \in [t_1, t_2] \} \mid e_1 \in r' \}}
\]
As a consequence, the time interval \([t_1, t_2]\) covering the vs can be
shifted such that either every revision is evaluated on its own or that multiple
are evaluated at a time.

Additionally, a threshold can be applied to filter out noise such that only
after a certain amount of shared revisions of two entities \(e_1 \in E, e_2 \in E\),
the revisions are added to the shared revisions \(R'\) to increase the relevance
of this \textit{evolutionary similarity} measurement \cite{tornhill2015crimescene}.

The details of the utilization of this approach as part of our devised methodology
is explained in chapters \ref{chap:rationale} and \ref{chap:extracting-coupling}.

\smaller
\begin{lstlisting}[caption=Example Git log, label=git-log, breaklines=true]
commit 7fa508ea483d45b037ee9507a6e92e5bb2118418
Author: Jakob Loehnertz <mail@jakob.codes>
Date:   Tue Jun 18 11:23:33 2019 +0200

    Add the average coupling modularity to the selectable ones

2   0   ../controller/analysis/metrics/clustering/ClusteringQualityAnalyzer.kt
1   0   ../model/metrics/ClusteringQuality.kt
6   3   ../frontend/src/App.vue
\end{lstlisting}
\normalsize



\section{Graph clustering} \label{sect:background-graph-clustering}

The term \textit{graph clustering}, sometimes also called \textit{community detection},
describes the process of partitioning a usually undirected
and unweighted or weighted graph into subgraphs \cite{lancichinetti2009community}.
Important to our work is that partitions on the input graph are calculated,
so that the resulting clusters do not overlap although a subset of
graph clustering algorithms exists that generates overlapping clusters.
A cluster is typically described as a group of vertices that are densely
interconnected via their \textit{intra-cluster edges} while being only sparsely
connected to other vertices outside of their own cluster via
\textit{inter-cluster edges} \cite{lancichinetti2009community, newman2004fast}.

\paragraph{Modularity}
The metric of \textit{modularity} measures the quality of a graph clustering
by comparing the interconnectedness of intra-cluster vertices to their
inter-cluster edges. It works on unweighted as well as weighted graphs
\cite{clauset2004modularity, blondel2008modularity}
and was devised in 2004 by Newman \cite{newman2004fast}:
\[
  Q =
  \frac{1}{2m}
  \sum \limits _{ij}{\bigg[ A_{ij} - \frac{k_i k_j}{2m} \bigg]}
  \delta (c_i, c_j)
\]
where
\begin{itemize}[noitemsep]
  \item \(A_{ij}\) is the edge weight between vertices \(i\) and \(j\),
  \item \(k_i\) and \(k_j\) is the sum of the edge weights attached to
        the vertices \(i\) and \(j\),
  \item \(2m\) is the sum of all edge weights,
  \item \(c_i\) and \(c_j\) represent the clusters of the vertices \(i\) and \(j\),
  \item \(\delta\) is a delta function
\end{itemize}
The metric ranges on a scale from \([0.0, 1.0]\), from no clustering structure
at all to a perfect clustering. Any given graph has a theoretic maximum in terms
of the achievable \textit{modularity}. However, it is rarely \(1.0\) and finding
the global maximum is proven to be an NP-complete optimization problem
\cite{brandes2006maximizing}. Nevertheless, the notion in literature is that
values over \(0.4\) already resemble a strong clustering structure
\cite{newman2004fast, fortunato2007resolution}.


\subsection{Renowned graph clustering algorithms} \label{subsect:renowned-graph-clustering-algorithms}

There are many graph clustering algorithms that were devised in this century
and are considered as state-of-the-art
\cite{lancichinetti2009community, fortunato2010community, danon2005comparing}.
Thus, in this subsection we discuss renowned graph clustering algorithms.
Furthermore, the way that we utilize the exhibited graph clustering algorithms
as part of this work is discussed further in chapter
\ref{chap:creating-service-recommendations}.

\paragraph{Girvan-Newman \cite{girvan2002community}}
The \textit{Girvan-Newman} algorithm is seen as one of the earliest, successful,
wide-spread graph clustering algorithm \cite{lancichinetti2009community}.
It was released by Girvan and Newman as part of their influential 2002 paper:
\textit{"Community structure in social and biological networks"} \cite{girvan2002community}.
Regardless of the title, their algorithm is generic and works on any graph network
as its input \cite{lancichinetti2009community}.
Their algorithm is performing hierarchical clustering which means that
every cluster that it creates can have recursive sub-clusters
again \cite{girvan2002community}.
Thus, this can alternatively be represented as a dendrogram \cite{newman2004fast}.

It iteratively removes edges with the highest
\textit{edge betweenness centrality}, which is based on
\textit{betweenness centrality} defined in 1977 by Freeman as
a metric that can be calculated for each vertex based on the idea that every
two vertices of a given graph have a shortest path that connects them,
the amount of how many of all those shortest paths go through any given vertex
represents its \textit{betweenness centrality} \cite{freeman1977set}.
As a consequence, Girvan and Newman adapted the idea for edges and coined it
\textit{edge betweenness centrality} \cite{girvan2002community}:
\[
  C(e) = \sum_{s, t \in V} \frac{\sigma(s, t \mid e)}{\sigma(s, t)}
\]
Here, \(\sigma(s, t \mid e)\) resembles the amount of shortest paths
between \(s\) and \(t\) that pass through \(e\) while \(\sigma(s, t)\)
is the amount of shortest paths between \(s\) and \(t\).

However, owing to the fact that their algorithm works hierarchically,
in the original implementation, the desired number of clusters has to be set
beforehand which causes the algorithm to cut edges until the set amount of
clusters is reached.

\paragraph{MCL \cite{vandongen2000graph}}
Two years before Girvan and Newman published their work, Van Dongen devised
the graph clustering algorithm \textit{MCL} \cite{vandongen2000graph}.
It works via random walks based on Markov chains by alternating two operations
on the input graph until it reaches a state of convergence regarding its
clustering structure \cite{vandongen2000graph}. These operations are
\textit{expansion} and \textit{inflation} \cite{vandongen2000graph}.

The first step of \textit{expansion} computes new probabilities for random walks
for every vertex pair in the graph. Owning to the fact that random walks within
clusters are usually of higher length as there are many edges to take, intra-cluster
vertex pairs usually receive higher probabilities \cite{vandongen2000graph}.

The second step, the \textit{inflation}, then boosts and demotes probabilities
further which over the course of many iterations will cause inter-cluster walks
to \textit{dry out} --- Van Dongen uses the terminology of \textit{flow} in fact
\cite{vandongen2000graph}. Consequently, this process will form a clustering
over time.

The strength of the \textit{inflation} can be set via an input parameter which
affects the granularity of the clustering and has therefore a similar problem
as the \textit{Girvan-Newman} algorithm. A solution to this issue is discussed
in chapter \ref{chap:creating-service-recommendations}.

\paragraph{Walktrap \cite{pons2005computing}}
The \textit{Walktrap} algorithm was devised in 2005 by Pons and Latapy
\cite{pons2005computing} and shares similarities with \textit{MCL}.
As its name implies, it tries to compute a graph clustering by utilizing short
Markov chain random walks and the idea that due to existing, albeit latent
clustering structure of a given graph, those random walks are more likely
to get \textit{trapped} in said clusters \cite{pons2005computing}.
%TODO: One could describe a little more here, maybe?!
The main improvement over \textit{MCL} is better runtime complexity
\cite{pons2005computing}.

\paragraph{Clauset-Newman-Moore \cite{clauset2004modularity}}
Two years after releasing the \textit{Girvan-Newman} algorithm, Newman
released another graph clustering algorithm \cite{newman2004fast}.
Its idea is based on maximizing the \textit{modularity} of the clustering.
However, shortly after, another improved version of his new algorithm
was published together with Clauset and Moore which is the
faster and generally improved version \cite{clauset2004modularity}.

As abovementioned, maximizing \textit{modularity} is an NP-complete
optimization problem which led Clauset et al. to solve it via a greedy heuristic.
Their algorithm starts off with putting every vertex of the given graph in its
own cluster, such that \(\{\{v\} \mid v \in V\}\).
Afterwards, in a maximum of \(\vert V \vert - 1\) iterations, the two clusters
that share a connection and whose amalgamation increases the \textit{modularity}
\(Q\) the most, respectively decreases it the least, are merged
\cite{clauset2004modularity}.

Eventually, this greedy algorithm will converge in a local or even global
optimum regarding the \textit{modularity} of the computed clustering
\cite{clauset2004modularity}.

\paragraph{Louvain \cite{blondel2008modularity}}
The \textit{Louvain} algorithm was devised in 2008 by Blondel et al. and is
named after their university \cite{blondel2008modularity}.
It is based on a similar premise to that of \textit{Clauset-Newman-Moore}.
The algorithm by Blondel et al. consists of two distinct phases.

Before beginning with the first phase, every vertex is placed in their own cluster.
The first phase then checks for each vertex with which adjacent vertex it can
merge clusters to reach a maximum increase in \textit{modularity}.
Compared to \textit{Clauset-Newman-Moore}, only if the \textit{modularity}
can be actually increased, the clusters are merged \cite{blondel2008modularity}.
All vertices are iterated over and checked repeatedly until a local optimum
is reached \cite{blondel2008modularity}.

As part of the second phase then, a new weighted graph is constructed with
the clusters detected in the first phase as the vertices.
The weighted edges between the new vertices are the sum of all vertices
the newly connected vertices are constructed out of.
Additionally, self-loops are attached to every vertex \cite{blondel2008modularity}.

Finally, the first phase is applied to the newly constructed graph again.
These two phases are then iteratively alternated until the increase in
\textit{modularity} ends or reaches a plateau \cite{blondel2008modularity}.
Hence, the \textit{Louvain} is a heuristic algorithm as well.

Fortunato et al. showed that \textit{modularity} optimization suffers
the problem of not being able to identify clusters of small sizes, the so called
\textit{resolution limit problem} \cite{fortunato2007resolution}.
The main improvements proposed by Blondel et al. specifially compared
to the algorithm by Clauset et al. are better runtime complexity, and a solution
to the \textit{resolution limit problem}, that \textit{Clauset-Newman-Moore}
faces, by leveraging the aforementioned self-loops \cite{blondel2008modularity}.

\paragraph{Infomap \cite{rosvall2008infomap}}
The \textit{Infomap} algorithm was devised in 2008 by Rosvall and Bergstrom
\cite{rosvall2008infomap}.
Its process bears a similarity to the \textit{Louvain} algorithm although
its core idea is entirely different. The similarity stems from the fact
that every vertex is also assigned to its own cluster in the beginning and
that cluster are then iteratively merged when this improves the metric that
the algorithm optimizes toward. However, compared to the \textit{Louvain}
algorithm, instead of \textit{modularity}, a \textit{map equation} is minimized.

Precisely, \textit{Infomap} represents the given graph as a random walk encoded
via Huffman coding \cite{huffman1952coding}.
Additionally, each cluster receives a prefix so that the codes for the vertices
can be reused in different clusters which decreases the required amount of bits
to represent the Huffman coding \cite{rosvall2008infomap}.

%TODO: One could describe a little more here, maybe?!

Finally, the phase that is reiterated over and over eventually tries to
change the clusters of the vertices such that the \textit{map equation}
representing the graph clustering is minimized \cite{rosvall2008infomap}.

\paragraph{Chinese Whispers \cite{biemann2006chinese}}
The \textit{Chinese Whispers} algorithm was released in 2005 by Biemann
\cite{biemann2006chinese}.
It mainly consists of an endless loop that is terminated once no better
clustering can be found anymore (i.e. a local or global optimum is reached)
or a predefined number of iterations has passed \cite{biemann2006chinese}.
At first, every vertex of the given graph is put into its own cluster.
Next, the vertices are iterated over in random order. Each vertex is moved
to the cluster that is shares the most edges with \cite{biemann2006chinese}.

Owing to the fact, that this algorithm is incorporating randomness,
it is non\hyp deterministic. Moreover, it yields a flat clustering
(i.e. non\hyp hierarchical) \cite{biemann2006chinese}.




\chapter{Related work} \label{chap:related}

The described problem of detecting latent boundaries within existing
application software for the purpose of decomposing it into microservices is
a sparse research domain, especially when compared to others
in computer science \cite{fritzsch2018monolith}.
Nevertheless, the existing research is categorized, rated and analyzed in the
following \textit{\nameref{subsect:literature-survey}}.

To add to this chapter, Fritzsch et al. conducted a literature study titled
\textit{From Monolith to Microservices: A Classification of Refactoring Approaches} (2019)
\cite{fritzsch2018monolith} which looked at research in the same domain as ours.
They conclude their research with:
\begin{displayquote}
\emph{"Our literature review has revealed a lack of systematic guidance on the
refactoring process for existing monolithic applications."}~\cite{fritzsch2018monolith}
\end{displayquote}
This underlines the relevance and importance of this area of research and the
notion that there is still room for improvement.
\newpage



\section{Literature survey} \label{subsect:literature-survey}

\begin{table}[ht!]
\def\arraystretch{1.55}
\begin{tabularx}{\textwidth}{|Y||Y|Y|Y|Y|Y|Y|Y|Y|Y|Y|}
 \hline
 \multicolumn{1}{|c||}{~} & \multicolumn{6}{c|}{Approaches} & \multicolumn{4}{c|}{Features}\\
 \hline\hline
 \rot{Citations~} & \rot{SE model analysis~} & \rot{Static analysis~} & \rot{Dynamic analysis~} & \rot{Data model analysis~} & \rot{VCS mining~} & \rot{External interface analysis~} &
 \rot{Evaluation~} & \rot{Evaluation by testing~} & \rot{Evaluation via metrics~} & \rot{(Available) Proof-of-concept~} \\
 \hline\hline
 \cite{gysel2016service}          & $\oplus$  & ~         & ~         & ~       & ~         & ~         & $\ominus$ & $\oplus$  & ~         & $\oplus$  \\\hline
 \cite{kruidenberg2018monoliths}  & $\odot$   & $\odot$   & $\odot$   & ~       & $\odot$   & ~         & $\odot$   & $\odot$   & $\ominus$ & $\oplus$  \\\hline
 \cite{mazlami2017extraction}     & ~         & $\odot$   & ~         & ~       & $\oplus$  & ~         & $\odot$   & $\oplus$  & $\odot$   & $\oplus$  \\\hline
 \cite{jin2018functionality}      & ~         & ~         & $\oplus$  & ~       & ~         & ~         & $\oplus$  & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{baresi2017microservices}   & ~         & ~         & ~         & ~       & ~         & $\oplus$  & $\oplus$  & $\oplus$  & $\odot$   & $\odot$   \\\hline
 \cite{chen2017monolith}          & $\oplus$  & ~         & ~         & ~       & ~         & ~         & $\odot$   & $\odot$   & ~         & $\ominus$ \\\hline
 \cite{escobar2016towards}        & ~         & $\oplus$  & ~         & ~       & ~         & ~         & $\odot$   & $\odot$   & $\odot$   & $\ominus$ \\\hline
 \cite{levcovitz2016towards}      & ~         & $\odot$   & ~         & $\odot$ & ~         & $\odot$   & $\ominus$ & $\ominus$ & ~         & $\ominus$ \\\hline
 \cite{knoche2018using}           & ~         & $\odot$   & ~         & ~       & ~         & ~         & $\ominus$ & $\ominus$ & ~         & $\ominus$ \\\hline
\end{tabularx}
\caption{Literature survey matrix}
\caption*{
  The signs used in the matrix  are defined as follows:
  \begin{itemize}[noitemsep]
      \item An empty cell depicts that the approach or feature of that column was not utilized
      by the work cited in that row.
      \item A circle with a dot ($\odot$) depicts that the approach or feature
      of that column was utilized by the work cited in that row without any outstanding execution
      or results in any direction or with results that were difficult to assess.
      \item A plus sign ($\oplus$) depicts that the approach or feature
      of that column was utilized by the work cited in that row in a successful way.
      \item A minus sign ($\ominus$) depicts that the approach or feature
      of that column was utilized by the work cited in that row in a subpar way.
  \end{itemize}
}
\label{table:literature-survey}
\end{table}

The matrix \ref{table:literature-survey} visualizes the surveyed literature
via classification. Six general approaches as well as four features were
extracted from the works. The approaches were devised by studying the
available literature on the subject.
In summary, other researchers were using one or more of these six approaches
to answer their research questions regarding microservice decomposition
out of an existing monolith. Thus, pure greenfield approaches were
not targeted in the literature survey.

The evaluative signs were assigned via our own intuition of the execution
and results as well as the evaluation of the works in other analyzed literature.
The latter approach was especially possible due to
the existing work by Fritzsch et al. \cite{fritzsch2018monolith}.


\subsection*{Approaches}

In the following paragraphs, the approaches used as matrix columns are explained in more detail.
\paragraph{SE model analysis}
The term \textit{Software Engineering (SE) models} subsumes non\hyp source code
artifacts out of the domain of SE.
For instance, this category includes use case models and entity relationship models (ERM).
\paragraph{Static analysis}
Static analysis covers any form of non\hyp runtime analysis of the given code base.
This includes dependency graphs, class graphs, connection of source code via shared terms
(i.e. words in the identifier of classes that might tie them together) etc.
\paragraph{Dynamic analysis}
Dynamic analysis is about analyzing the existing monolithic software during its execution.
This covers profiling it, tracing data inside of the application as well as analyzing
access logs such as web traffic logs.
\paragraph{Data model analysis}
Owing to the fact that one principle of the microservices architecture is about
separating data stores \cite{ms-microservices}, the approach to analyze
the data models (e.g. schemes) of existing database tables is, generally speaking,
an important one. This approach covers direct analysis of database tables
as well as the analysis of \textit{object relational mappers} (ORM)
if those were already in use in the existing application.
\paragraph{VCS mining}
The repositories of existing solutions are usually part of a
version control system (VCS) such as Git. Consequently, revisions and merges
can be analyzed to e.g. determine the coupling of certain files, modules, or classes.
\paragraph{External interface analysis}
The external interfaces (i.e. the Web-APIs) of existing solutions can be analyzed to get
a sense of existing modularization and cohesion within the solution.


\subsection*{Features}

In the following paragraphs, the features used as matrix columns are explained in more detail.
\paragraph{Evaluation}
This column just depicts if the cited source of a row evaluated their work in any form.
\paragraph{Evaluation by testing}
Since this literature survey was focused on works that research assisted or semi-automated
microservice decomposition, theoretically, the works could be evaluated with real-world or
even artificial data such as large open source projects built with a monolithic architecture.
\paragraph{Evaluation via metrics}
Owing to the fact that the area of research of this project is still rather young and sparse
\cite{jin2018functionality}\cite{fritzsch2018monolith}, custom as well as renowned metrics
for the evaluation are a welcomed addition to the research in this area.
Additionally, evaluation via metrics improves the overall credibility of a work.
\paragraph{(Available) Proof-of-concept}
This feature is somewhat tied to the \textit{Evaluation by testing} one as oftentimes
a prototype (i.e. proof-of-concept) is built to actually test the designed
methodology or algorithm. Moreover, a positive or a negative execution respectively
was awarded if the prototype was publicly available (e.g. as open source software)
since it is otherwise impossible to assess.



\section{Topic relations} \label{subsect:topic-relations}

Following the approaches and features extracted in the previous section,
the methodologies of the works that are the closest to ours are observed with greater detail.

Gysel and K{\"o}lbener \cite{gysel2016service} were the first to release work in
this area of research that is in the same vein to ours in the sense that a
complete methodology is developed to convert a certain input to microservice
recommendations. They initially devised the idea to build a graph out of this
input that is then clustered using a graph clustering algorithm to create clusters
that reflect the microservice recommendations.
Future work in the domain \cite{mazlami2017extraction}\cite{kruidenberg2018monoliths}
together with ours is following this idea.
However, not every analyzed work uses this approach.
Therefore in this subsection, only works that do use it are analyzed in detail
as they relate closely in topic.


\subsection{Analyzed inputs}

Gysel and K{\"o}lbener used \textit{SE model analysis} as their only approach ---
a type of input that is unlikely to be available or
can be created with a lot of manual work, as they acknowledge
themselves in their original work \cite{gysel2016service}.
This fact decreases the actual usability of the devised methodology and leaves
room for improvement.

Kruidenberg \cite{kruidenberg2018monoliths} built on top of the work and
especially the implementation of Gysel and K{\"o}lbener by extending their
\textit{SE model analysis} input with \textit{Dynamic analysis} input.
Precisely, they model the input, that the implementation of Gysel and K{\"o}lbener
requires and generates out of \textit{SE models}, from data retrieved
out of \textit{Static analysis}, \textit{Dynamic analysis}, and \textit{VCS mining}.
Consequently, this approach mainly solves the problem that the original approach
had regarding the availability and respectively the ease of creation
of supported input formats.

Comparatively, Mazlami \cite{mazlami2017extraction} devised their completely
custom methodology. The foundation is still based on building and then clustering
a graph but they chose \textit{Static analysis}, and \textit{VCS mining} as
their approaches of choice. From within the domain of static program analysis,
semantically analyzing source code was devised and implemented as input together
with mining evolutionary data out of version control system logs.

Instead, we utilize three distinct input dimensions for constructing
a more precise weighted graph which can then be clustered.


\subsection{Graph clustering}

Regarding the graph clustering, Gysel and K{\"o}lbener \cite{gysel2016service}
only utilized two graph clustering algorithms both of which have major
shortcomings specific to this area of research.
The Girvan-Newman algorithm \cite{girvan2002community} requires the
amount of clusters beforehand which is not ideal for recommending
microservice candidates as the software engineers prospectively using
such a tool can at best estimate that amount.
Additionally, one advantage of automated microservice decomposition recommendations
is that the algorithm might give recommendations that a human engineer would
not have considered which also affects the granularity and therefore the
amount of services (i.e. clusters).
The Leung algorithm \cite{leung2009community} on the other hand has the
disadvantage that it is non\hyp deterministic which might cause completely
different recommendations with every execution.

Consequently, Kruidenberg \cite{kruidenberg2018monoliths} uses the same
algorithms as his work is an extension of the one of Gysel and K{\"o}lbener.

Mazlami \cite{mazlami2017extraction} leveraged the notion of the
minimal spanning tree (MST) of the input graph to perform the clustering.
Thus, their methodology is not based on state-of-the-art graph clustering algorithms
but a custom approach instead. They implemented an algorithm
that iteratively calculates the MST and removes the edge with the lowest similarity
from the graph. However, this approach has the same problem as the
Girvan-Newman algorithm \cite{girvan2002community} that Gysel and K{\"o}lbener
utilized in the sense that it requires the desired amount of services beforehand.

Instead, we show how to leverage heuristic optimization of the clustering and
therefore the recommendations toward a numeric metric via a fitness function
while incorporating a large set of state-of-the-art graph clustering algorithms
to be able to compare them and assess which one performs the best for a given input.
Thus, this removes the problem of having to define the desired amount of microservices
a priori.


\subsection{Evaluative metrics}

Gysel and K{\"o}lbener \cite{gysel2016service} just evaluated their
initially set requirements on a custom scale of 1--3 together with a devised
questionnaire to assess the quality of the recommendations.
The questionnaire however follows no numeric metrics and just allows answers
in a boolean manner which makes it more subjective.

Mazlami \cite{mazlami2017extraction} devised a set of six metrics that measure
the output of their methodology. Although some of them do not have the notion
of if a higher value is better or worse (e.g. \textit{Average Contributors per Microservice}).

Kruidenberg \cite{kruidenberg2018monoliths} used a combination of metrics from
the works of Gysel and K{\"o}lbener \cite{gysel2016service} and
Mazlami \cite{mazlami2017extraction}.

Instead, we devise an extensive set of numeric metrics that offers
an objective and quantifiable view on the resulting microservice recommendations
to be able to evaluate not only our methodology but the recommendations them self as well.




\chapter{Mapping microservice metrics to software quality metrics} \label{chap:rationale}

As the focal point of our work is to construct a graph-theoretic representation
of the input monolithic application to cluster it afterwards,
a rationale how the measured dimensions of the input map to the output
is deemed necessary to corroborate our research.



\section{Design principles of microservices}

Thus, said rationale was devised leveraging a few \textit{design principles}
of microservices \cite{ms-design-principles, ms-fowler, newman2015building}.
We looked at what we deemed to be the three most important ones:
\begin{enumerate}
  \item Services should keep inter-service communication to a minimum
  \item One service should only handle one task
  \item The services should have independent software development life cycles
\end{enumerate}
Following this, these three items are referred to via their number in the
above enumeration. Moreover, every item is discussed in more detail below.


\subsection{Principle 1}
As Newman and others lay out \cite{newman2015building, alshuqayran2016systematic},
very \textit{chatty} microservices are not desired due to three reasons.

Firstly, it can mean that the two services are too tightly coupled with
the solution that it might have been better to merge them together into one
\cite{newman2015building}.

Secondly, the stability is generally worse as the architecture becomes
more brittle since it always has to be assumed that inter-service communication
via a network just fails due to erroneous configuration, a temporary downtime,
or congestion \cite{newman2015building, alshuqayran2016systematic}.

Thirdly, the performance suffers compared to native method invocations e.g.
as part of a monolithic application. Communication over a network always
involves latency and the serialization and deserialization
of each request and response (i.e. when utilizing HTTP) is a costly action
in terms of runtime \cite{newman2015building}.


\subsection{Principle 2} \label{subsect:ms-principle-2}
The second principle is ambiguous without any further elaboration owing to
the fact that the term \textit{task} has no notion of size or complexity
attached to it.

Nevertheless, this idea can be seen as the focal point of the whole microservices
architecture style due to the term itself --- \textit{micro} to begin with
is vague, as Fowler discusses:
\begin{displayquote}
\emph{"[...] its name does lead to an unfortunate focus on the size of service,
and arguments about what constitutes “micro”.
In our conversations with microservice practitioners,
we see a range of sizes of services."}~\cite{ms-fowler}
\end{displayquote}
To make it entirely clear, the objective size of a microservice is not very
important regarding the second principle. Instead, the idea of the
\textit{bounded context}, introduced by Evans in his influential 2004 book
\textit{Domain Driven Design} \cite{evans2004ddd}, is widely accepted as
a helpful methodology to devise the boundaries of each microservice
in a given application \cite{newman2015building, ms-fowler}.

Thus, a \textit{task} has no distinct upper limit regarding its size.
One service should encapsulate one \textit{bounded context} while trying to
keep them as small as reasonable possible to still satisfy the idea of having
\textit{microservices} so that size, complexity, and maintenance stay manageable
\cite{newman2015building, ms-fowler}.

\paragraph{Bounded contexts}
As mentioned, Evans created the idea of the \textit{bounded context} defining
it as follows:
\begin{displayquote}
\emph{"A description of a boundary
(typically a subsystem, or the work of a particular team)
within which a particular model is defined and applicable."}~\cite{evans2014ddd}
\end{displayquote}
while he defines \textit{model} as:
\begin{displayquote}
\emph{"A system of abstractions that describes selected aspects of a domain
and can be used to solve problems related to that domain."}~\cite{evans2014ddd}
\end{displayquote}
Precisely, the idea of the \textit{bounded context} is about what parts of the
domain \textit{model} are independent enough to be \textit{bounded contexts},
while it is even more important to define a clear interface regarding
which parts of the \textit{model} should not leave the \textit{bounded context}
and which parts have to be shared externally for the application to function
and properly mirror its domain \cite{newman2015building}.


\subsection{Principle 3}
Since microservices are attributed to \textit{Conway's law}
\cite{ms-fowler, dragoni2017microservices, conway1968law}
in the sense that this architectural style lends itself well to letting
separate teams handle one or multiple microservices \cite{newman2015building},
it keeps the services decoupled apart from data and interfaces by having
independent software development life cycle.

The more dependent the life cycles are, the more coupled the services become
and hence the longer it will take to apply feature additions, changes,
or bug fixes which can hinder the development progress.

As long as the interfaces of the services are well designed,
as long as they do not change, other developers in other teams do not have
to worry much that services they maintain get incompatible with other services
if they or the other services change internal logic --- it is in its core
nothing more than the notion of proper \textit{encapsulation} \cite{ms-fowler}.



\section{Selecting and extracting software quality metrics}

All of the aforementioned principles will be violated if the boundaries of the
microservices are chosen poorly. Thus, we propose a semi-automated methodology
that extracts information out of the given monolithic application and devises
a \textit{good} decomposition regarding those boundaries.

Following this, we decided to map the \textit{principles of microservices} to
dimensions of the input into our devised methodology.
This gives a rationale for how the eventual output of our methodology can
be qualitative based on the fact that the input dimensions were carefully
chosen based on what literature defines as \textit{good} based on the principles.
Essentially, the input is mapped to microservices architecture best practices
which allows us to reason about how and why the output of our methodology
makes any sense, all based on the fact that it processes input mapped to said
best practices.

Eventually, we found that three main types of coupling in software systems map
to the abovementioned principles:
\begin{enumerate}
  \item Dynamic coupling
  \item Semantic coupling
  \item Evolutionary coupling
\end{enumerate}
The term \textit{coupling} is defined as:
\begin{displayquote}
\emph{"A measure of how closely connected two routines or modules are;
the strength of the relationships between them."}~\cite{swebok}
\end{displayquote}
Precisely, the three types of coupling above are subcategories of the notion of
\textit{coupling}. Moreover, these types as well as how they map to the
\textit{principles of microservices} are showcased in figure
\ref{fig:principles-of-microservices} and the subsections below.
Furthermore, the details of the utilization
of this mapping as part of our methodology are discussed in chapter
\ref{chap:extracting-coupling}.

\begin{figure}[htbp]
\centering
\includegraphics[width=.90\textwidth]{principles-of-microservices}
\caption{Mapping design principles of microservices to coupling types}
\caption*{
  We detected that three of the most important design principles of the
  microservices paradigm maps directly to three main types of coupling.
}
\label{fig:principles-of-microservices}
\end{figure}


\subsection{Dynamic coupling}

The term refers to any metric of coupling that measures how coupled
components are during their runtime hence it is recorded during the execution
of a given application which ties it to \textit{dynamic program analysis}.

For instance, counting how often a certain component calls or accesses another
one is a metric described and covered in \ref{subsubsect:call-graph-dynamic}
as dynamic call graph generation based on \textit{execution counts}.
To clarify, a weighted and directed graph can be constructed when gathering
this \textit{dynamic coupling} metric for every single \textit{execution count}.
Hence, the weighted edges of the graph symbolize the \textit{execution counts}.

However, we opted not to make the constructed graph directed as graph clustering
is usually not applied to directed graphs \cite{lancichinetti2009community}.
Moreover, the other coupling types also only yield undirected graphs which
would make a combined analysis on all of them non\hyp functional.

Finally, the first principle maps to this type of coupling due to the fact that
the \textit{dynamic coupling} information can be utilized to keep
communicative components from being spread over multiple microservices
(i.e. being clustered together), which would otherwise violate the first principle.


\subsection{Semantic coupling}

Just like with the \textit{dynamic coupling}, an undirected and weighted graph
is constructed out of \textit{semantic coupling} information.
Precisely, it is measured how semantically similar two components are which
causes the weighted edge in the resulting graph that connects their vertices
to hold this similarity value.

Finally, the second principle maps to this type of coupling due to the fact that
the \textit{semantic coupling} information can be utilized to keep
components that cover a similar (or the same) task from being spread over
multiple microservices (i.e. being clustered together),
which would otherwise violate the second principle.


\subsection{Evolutionary coupling}

Again, an undirected and weighted graph can be constructed out of the
\textit{evolutionary coupling} information.
To be more precise, if two components change together more often it can be
inferred that their life cycles are more or less coupled.
To quantify this, the approach described in \ref{subsect:evolutionary-similarity}
is leveraged to precisely calculate how \textit{evolutionary coupled} components
are (i.e. how much their life cycles overlap) which is then represented
by the edge weights in the resulting weighted graph.

Finally, the third principle maps to this type of coupling due to the fact that
the \textit{evolutionary coupling} information can be utilized to keep
components that currently share the same software development life cycle
from being spread over multiple microservices (i.e. being clustered together),
which would otherwise violate the third principle.




\chapter{Extracting coupling information from software} \label{chap:extracting-coupling}

An important argument of our approach is that the coupling information
exhibited in chapter \ref{chap:rationale} are extracted out of the input
monolithic application automatically without any further user input.
In this chapter it is detailed which inputs need to be supplied and
how the three types of coupling are extracted out of them. Additionally,
a static program analysis is conducted as covered in this chapter.

Sometimes it is not possible to provide every of the four inputs which is why
at least any three out of the four have to be provided while our devised
methodology can still conduct its analysis.

Finally, our methodology is focused on object-oriented programming,
hence we will from now on use the term \textit{class} instead of \textit{entity}
for describing the vertices of the constructed weighted graphs.
Moreover, the following descriptions are on the example of Java applications,
however in chapter \ref{chap:implementation} we exhibit how other
languages and platforms can be added to the implementation.



\section{Static coupling extraction} \label{sect:static-coupling-extraction}

As discussed in more detail in chapter \ref{chap:software-as-weighted-graph},
we opt for also constructing a full static call graph which is covered
in subsection \ref{subsubsect:call-graph-static}.


\subsection{Walk\hyp based approach}

We utilize an approach that starts with the \code{main} method or methods of
the given application and then recursively walks down the classes and their
methods that are instantiated or invoked. Consequently, a graph with
possible cycles is built.
Both, class instantiations as well as method invocations can be registered
although as part of our methodology, both  will result in an edge connecting
the instantiating or invoking class vertex to the one it is instantiating
or invoking --- the algorithm \ref{algo:class-walk} depicts this process.

\begin{algorithm}[ht]
\caption{Recursive walk trough classes and methods}
\label{algo:class-walk}
  \BlankLine
  \KwData{
    The source code or compiled code of an application
  }
  \KwResult{
    A graph with the classes as vertices and their instantiations and
    invocations toward others as the edges
  }
  \BlankLine
  $cg$ $\leftarrow$ empty list of pairs of classes representing the call graph\;
  $av$ $\leftarrow$ empty set for classes that have already been visited\;
  $cq$ $\leftarrow$ empty queue for classes that should be visited\;
  \BlankLine
  push the \code{main} methods of the application onto queue $cq$\;
  \BlankLine
  \While{class queue $cq$ is not empty}{
    $vc$ $\leftarrow$ pop class to visit next from queue $cq$\;
    \ForEach{method $vm$ of class $vc$}{
      \ForEach{class $ic$ instantiated or invoked from method $vm$}{
        add pair of $vc$ and $ic$ to call graph list $cg$\;
        \If{$ic$ is not in set of already visited classes $av$}{
          push class $ic$ onto class queue $cq$\;
          add class $ic$ to set of already visited classes $av$\;
        }
      }
    }
  }
  \BlankLine
  \Return{call graph list $cq$}\;
\end{algorithm}

For Java applications, both source code as well as compiled bytecode can
be utilized as the input to algorithm \ref{algo:class-walk}.
For both approaches, libraries exist that can access and iterate over
the classes and methods as detailed in chapter \ref{chap:implementation}.

The resulting static call graph is theoretically directed in the manner that it
gets constructed in algorithm \ref{algo:class-walk}, the first class in the class
pairs that are getting returned by it is the vertex the edge is pointing away from.
Furthermore, it would be possible to count how often a class instantiates or
invokes another and accumulate this value as a weighted edge.

However, we opted to utilize neither directions nor weights
and instead to construct a regular undirected, unweighted graph.
Owing to the fact that the graphs resulting from the the other coupling types
are not directed, using a directed graph for this coupling type makes no sense.
Moreover, the weights associated with a \textit{static coupling} graph
do not have much meaning since the range of the weights is not very significant
as certain algorithms may instantiate a lot of other classes,
yet this does not strictly increase their coupling.
Specifically when compared to the other coupling types, potential edge weights
on the \textit{static coupling} graph are not very relevant, thus this graph
is plainly used for being able to obtain a final graph that covers the
entirety of the input monolithic application, as mentioned earlier.



\section{Dynamic coupling extraction}

To extract the \textit{dynamic coupling} information, two general approaches
were detected and evaluated. Both have advantages and disadvantages which
are discussed in the following subsections. However, for both approaches,
the application has to be executed which involves a lot more overhead compared
to the previously exhibited \textit{static coupling} graph construction.

As part of our methodology, we decided that the user can choose which option
they want to use:
\begin{itemize}[noitemsep]
  \item The \textit{sampling\hyp based} approach has lower fidelity but
        also less impact on the runtime
  \item The \textit{instrumentation\hyp based} approach has high fidelity but
        also a large impact on the runtime
\end{itemize}
Our methodology works with both approaches with the difference that the results
with an \textit{instrumentation\hyp based} input are of better quality due to the
higher input fidelity. This is further detailed in the chapter on
\textit{\nameref{chap:implementation}}.


\subsection{Sampling\hyp based approach}

The first approach is more concerned with observing (i.e. monitoring)
the running application from the outside. The typical method to do so is
a \textit{sampling\hyp based} application profiling. This means that with a preset
frequency, every \(n\) milliseconds, a sample is recorded regarding what the
application is currently doing. This concerns actions such as
the memory usage, thread usage, which method is invoked how often, etc.

Focusing on the \textit{Java Virtual Machine} (JVM) as we target it with our
implementation, it ships with a built-in profiler,
called \textit{Java Flight Recorder} (JFR).
The JFR is a \textit{sampling\hyp based} profiler that is shipped
with \textit{OpenJDK} versions starting from 11 \cite{openjdk-jfr}.
For other platforms and languages, different profilers are available or built-in,
yet the idea to record profiling data at a certain sampling rate
stays the same, implementation details however differ from runtime to runtime.

The advantage of this approach is that the overhead on the runtime is kept to
a minimum. It is highly dependent on the implementation but in the example of
the JVM, the JFR only has a claimed overhead of about 1\% \cite{openjdk-jfr}.

The downside is that the fidelity of the recordings is highly dependent on the
sampling rate. The lower the sampling rate, the more likely it just misses
certain events (e.g. method invocations) which leads to the situation that
there is no certainty how much it actually recorded --- yet, this is true
for every \textit{sampling\hyp based} approach and an inherent problem of them.
However, the higher the sampling rate gets, the higher is the overhead for
recording sampling more while still not eliminating said inherent problem.


\subsection{Instrumentation\hyp based approach}

Compared to the \textit{sampling\hyp based} approach described above,
an \textit{instrumentation\hyp based} approach can reach complete coverage of
theoretically every event happening during the runtime.
We are only concerned with method invocations, thus an
\textit{instrumentation\hyp based} approach in that case encompasses logging
every single method entry together with which method invoked it.

The disadvantage of this approach is that the impact on the runtime performance
is immense owing to the fact that every single method invocation is logged.

Generally, there are multiple ways how to implement
\textit{instrumentation\hyp based} profiling however the most common way
is some form of aspect-oriented programming (AOP) \cite{kiczales1997aop}.

The idea of AOP is to essentially inject extra functionality for
\textit{cross-cutting concerns}, which describes operations that are not
necessarily limited to a certain feature, such as logging \cite{kiczales1997aop}.
The idea itself is simple however, first the user defines \textit{cutting points},
which behave like patterns that are being matched on the source code,
for instance: \textit{at every method entry}.
They define where in the code base the extra functionality should be injected.
In the next step, the user defines source code that is then injected at every
\textit{cutting point} either in a static way at compile time or,
in the case of bytecode\hyp based languages, in the moment the bytecode is loaded
into the runtime environment \cite{kiczales1997aop}.
At runtime, at every \textit{cutting point}, method entries in our case,
the injected piece of logging code is executed just as the rest of the code
\cite{kiczales1997aop}.



\section{Semantic coupling extraction}

Similarly to the \textit{static coupling} again, the \textit{semantic coupling}
extraction only needs the source code itself. Importantly, human-readable,
not yet compiled source code is necessary as the semantics are lost in compiled
code since it is not human-readable anymore.
We implemented an approach from scratch that conducts the  steps regarding
semantic similarity calculation from subsection \ref{subsect:semantic-similarity}:
\begin{enumerate}[noitemsep]
  \item Tokenization
  \item Stop word filtering
  \item Normalization
  \item Vectorization
  \item Relevance weighted via \textit{tf-idf}
  \item Latent semantic indexing (LSI)
\end{enumerate}
As detailed in subsection \ref{subsect:semantic-similarity},
specifically the first two steps are different when analyzing source code
compared to traditional natural language.


\subsection{Natural language processing on source code}

Just as the other input dimensions, we implemented our devised methodology
on the example of Java applications, hence this subsection will focus on
Java source code. Other rules would apply to other languages although
the general concept remains the same.

Regarding the first step, the \textit{tokenization}, an important difference to
the \textit{tokenization} of truly natural language is encountered.
Owing to the fact that line breaks are an important tool of syntactic separation
in programming, they are treated separately, this becomes apparent with an example.
Java source code as well as many other higher level programming languages have
the notion of recursive packages and importing them or parts of them into the
source code of other packages.
Consequently, source code such as the following example \ref{packages-and-imports}
would include package paths in the term list of the respective document
which would distort the result of the \textit{semantic coupling} analysis.
Thus, every line is parsed separately and language-specific keywords such as
\code{package} or \code{import} trigger that the line get discarded.

\smaller
\begin{lstlisting}[caption=Packages and imports in Java, label=packages-and-imports, language=Java, breaklines=true]
package codes.jakob.semanticcoupling.parsing

import codes.jakob.semanticcoupling.model.Document
import codes.jakob.semanticcoupling.model.NaturalLanguage
import codes.jakob.semanticcoupling.model.ProgrammingLanguage
import codes.jakob.semanticcoupling.model.Term
import codes.jakob.semanticcoupling.normalization.Lemmatizer
import codes.jakob.semanticcoupling.normalization.Normalizer
import codes.jakob.semanticcoupling.normalization.Stemmer
\end{lstlisting}
\normalsize

Next, every line of the source code is stripped of non\hyp word characters
(i.e. \code{W} from regular expressions) which is every character
but a-z, A-Z, 0-9, and the underscore character \cite{ieee1992posix}.
Before that step, explicit strings (i.e. text in double quotes) are separately
saved so they do not lose their meaning when stripping the double quotes.
Additionally, the explicit strings as well as every other text that is still
left then and matches a camel-case pattern is split into a new term at every
\textit{camel hump}. Moreover, numeric characters as well as single characters
are filtered out, as both provide no relevant semantic meaning.

Furthermore, as part of the second step, the \textit{stop word filtering},
a list with stop words specific to the Java language is used to filter out
terms that are contained in the list. The list was crafted by us in line with
the official Java language specification \cite{oracle-java11}.
Examples for terms that are classified as stop words include:
\begin{itemize}[noitemsep]
  \item \code{class}
  \item \code{public}
  \item \code{static}
  \item \code{void}
\end{itemize}

Finally, everything that was not filtered out until this point is split
at white space characters (i.e. \code{t} from regular expressions)
\cite{ieee1992posix}.
Additionally, every resulting term is converted to lower case so that
the subsequent steps handle the same term in different cases as
semantically indifferent.

The rest of the steps is conducted just as described in subsection
\ref{subsect:semantic-similarity} since after the second step,
the following steps work on term lists as part of documents of the corpus
\(t_i \in d_i, d_i \in C\), which, at that point, are regular natural language.



\section{Evolutionary coupling extraction} \label{sect:evolutionary-coupling-extraction}

The last input dimension, the \textit{evolutionary coupling}, needs different
input. Instead of the application code itself, a log file generated by the
utilized version control system (VCS) is required.
This mandates that a VCS is used during the development of the input application.


\subsection{Mining version control system data}

The approach leveraged to mine VCS data is modeled after the methodology of
Tornhill \cite{tornhill2015crimescene} which is exhibited in subsection
\ref{subsect:evolutionary-similarity}.

Theoretically, every VCS that has the notion of revisions that include files
and that is able to generate a log file which details past revisions together
with the files involved, can be used to extract \textit{evolutionary coupling}
information. A few example VCS that we also support as part of our implemented
methodology detailed in chapter \ref{chap:implementation}:
\begin{itemize}[noitemsep]
  \item \textit{Git}
  \item \textit{Subversion}
  \item \textit{Mercurial}
  \item \textit{Perforce}
  \item \textit{Team Foundation Server}
\end{itemize}
All of the above also support to only generate a partial log for a certain
time period. However, \textit{Perforce} and \textit{Team Foundation Server}
only support to log the last \(n\) revisions compared to a time\hyp based range
\cite{perforce, team-foundation-server}.

A caveat with this approach is that by default, the log files will only
include files. Owing to the fact that in applications written in Java,
one file is also equivalent to one class, we can generate the coupling graph
without any changes to the log format. Nevertheless, it would be possible
to extend the \textit{evolutionary coupling} information extraction to support
languages that might keep multiple entities (i.e. the vertices in the graph)
in the same file by generating more detailed log files on a line level within
the files. However, this is not covered by our work as we focus on
Java applications for our initial implementation.

After parsing the input log file, differing based on the utilized VCS,
the process described in subsection \ref{subsect:evolutionary-similarity}
generates the weighted \textit{evolutionary coupling} graph with the same
structure as the other ones such that the vertices \(v_i \in V\) are the classes
extracted from the VCS log file while the edges \(e_i \in E\) are resembling
that two classes have overlapping software development life cycles.
The weight of the edges is a percentage that indicates how much the
life cycles overlap \(W(e_i) \in [0.0, 1.0]\).

Finally, as discussed in subsection \ref{subsect:evolutionary-similarity},
we use a threshold for this extraction step to filter out noise.
The values are constants that can be changed while the defaults that we employ are:
\begin{itemize}[noitemsep]
  \item Minimum shared revisions between entities: 1
  \item Time period \([t_1, t_2]\): Every day from \([00:00, \ 23:59]\)
\end{itemize}
For clarification, figure \ref{fig:vcs-log-timeline} depicts an example
timeline to show how the \textit{evolutionary coupling} is calculated
via a given a VCS log.

\begin{figure}[htbp]
\centering
\includegraphics[width=.96\textwidth]{vcs-log-timeline}
\caption{An example timeline of a VCS log}
\caption*{
  A textual VCS log is mined for \textit{evolutionary coupling} information
  by analyzing how many percent of the shared revisions of two entities
  do overlap. The figure specifically depicts the usage of time ranges
  and thresholds to obtain sound data.
  The solid lines are depicting revisions.
  The dotted lines are depicting midnight in between two days on the timeline.
  In the example, according to our explanation in
  \ref{sect:evolutionary-coupling-extraction}, the \textit{evolutionary coupling}
  values would be as follows (the values are rounded down):
  \begin{center}
  \begin{tabular}{|c|c|c|c|}\hline
    \backslashbox{To}{From}
    ~   & A     & B     & C     \\\hline
    A   & --    & 0.55  & 0.50  \\\hline
    B   & 0.83  & --    & 0.87  \\\hline
    C   & 0.66  & 0.77  & --    \\\hline
  \end{tabular}
  \end{center}
}
\label{fig:vcs-log-timeline}
\end{figure}




\chapter{Representing software as weighted graphs} \label{chap:software-as-weighted-graph}

Our methodology works by clustering a weighted graph hence we need to construct
it first. As discussed in chapter \ref{chap:rationale}, three general
coupling types were chosen to be leveraged for devising the microservice
candidates recommendations.
Precisely, the input monolithic application gets analyzed from the view of
each of these coupling types, called dimensions from now on.



\section{Building a weighted graph for each input dimension}

Out of each of the input dimensions, a weighted graph is built with the classes
as the vertices \(v_i \in V\) and the coupling values between the classes
as the weighted edges \(e_i \in E\) representing a percentage
\(W(e_i) \in [0.0, 1.0]\) --- resulting in a weighted graph \(G = (V, E)\).
Consequently, we receive three separate weighted graphs with the different
input dimensions as the edge weights.

However, the other extraction methods (i.e. dimensions) covered in the
sections below entail one problem that would would discredit
our whole methodology --- they oftentimes do not cover the entirety of the
input monolithic application in regard to featuring every class of it
as vertices in the resulting weighted graph.
As a consequence, it is possible that all three weighted graphs constructed
out of the three coupling types are not complete in the sense that they cover
the entirety of the input application. As this would defeat the purpose of
our approach, an additional static call graph is constructed to be certain
that the final graph covers the whole application.



\section{Merging input dimensional graphs into combined one}

Finally, four graphs are constructed out of the four input dimensions
that need to merged into a combined one so that it can be clustered eventually.
Essentially, this step encompasses a set-theoretic union
of the four graph tuples \(G_i = (V, E)\) such that:
\[
  G_{combined} = G_{static} \cup G_{dynamic} \cup G_{semantic} \cup G_{evolutionary}
\]
Importantly, this step utilizes unions owing to the fact
that all four input dimensions have downsides in producing a graph
that covers the whole application.
Nevertheless, they complement each other since they cover separate important
aspects of detecting the distinct types of coupling.

Static program analysis has the shortcoming that certain aspects of a written
application are only known at runtime such as the usage of
\textit{reflection} \cite{landman2017reflection}.
On the other hand, dynamic program analysis entails the problem that
it is by no means guaranteed that every class of the executed application
is called or even instantiated --- the resulting graph is highly dependent
on the workload while the dynamic analysis is conducted.
A similar argument is true for the last two input dimensions.
Oftentimes, not the entire application is covered due to thresholds that
are introduced to filter out noise, which is discussed in subsection
\ref{subsect:evolutionary-similarity}.

Consequently, unions ensure that the entirety of the given
monolithic application is covered as part of the graph-theoretic representation.
Precisely, all vertices from each of the four graphs are added to the combined one:
\[
  V_{combined} = V_{static} \cup V_{dynamic} \cup V_{semantic} \cup V_{evolutionary}
\]
For the edges, the merging is twofold. The edges themselves can just be all
added to the combined graph to connect the vertices \(V_{combined}\) according
to the four input graphs:
\[
  E_{combined} = E_{static} \cup E_{dynamic} \cup E_{semantic} \cup E_{evolutionary}
\]
However, the edge weights should not just be added up as all of them resemble
percentages. Instead, we use a modifiable \textit{edge weighting formula} which
is defining factors for each input dimension to increase or decrease its importance
in calculating the combined weight of each edge \(\forall e_i \in E_{combined}\)
in the combined graph \(G_{combined}\). Owing to the fact, that the
\textit{static coupling} dimension yields not edge weights as covered in section
\ref{sect:static-coupling-extraction}, it is excluded from
the \textit{edge weighting formula}:
\small
\begin{equation*}
\begin{split}
  W_{combined}(wf_{dynamic}, wf_{semantic}, wf_{evolutionary}) = \\
  \frac{
    wf_{dynamic} \times W(e_{dynamic}) +
    wf_{semantic} \times W(e_{semantic}) +
    wf_{evolutionary} \times W(e_{evolutionary})
  }{
    wf_{dynamic} +
    wf_{semantic} +
    wf_{evolutionary}
  }
\end{split}
\end{equation*}
\normalsize
In the above formula, the weight factors \(w_x\) can then be substituted
with custom factors. Our default setting is an average with all the
weight factors being \(1\).




\chapter{Creating microservice recommendations} \label{chap:creating-service-recommendations}

Once the combined graph \(G_{combined}\) has been constructed, the next step is
to apply graph clustering to it such that groups of vertices that have strong
coupling, resembled by the combined edge weights
\(W_{combined}(wf_{dynamic}, wf_{semantic}, wf_{evolutionary})\) in between them,
are more likely to end up in the same cluster.
These clusters would then mirror the notion of the \textit{bounded contexts},
introduced in subsection \ref{subsect:ms-principle-2}.
Thus, each cluster would then resemble one microservice candidate recommendation.

Consequently, the question arises which approach to take in clustering the
constructed graph \(G_{combined}\) at hand, respectively which graph clustering
algorithm to choose, to as optimally as possible devise the recommendations.



\section{Survey of graph clustering algorithms}

The idea of graph clustering was exhibited in the \textit{\nameref{chap:background}}
chapter together with a range of available, renowned algorithms.
A variety of graph clustering algorithms exists with a variety of different
goals, mechanisms, and features
\cite{lancichinetti2009community, fortunato2010community, danon2005comparing}.
Consequently, a dozen renowned graph clustering algorithms were evaluated and
assessed regarding their usability for our work.
A total of nine renowned graph clustering algorithms were chosen and classified
in table \ref{table:graph-clustering-survey}.
The way that classifications are negated was chosen so that $\oplus$
is a positive feature regarding the utilization as part of our work.

Eventually, the first seven algorithms from table \ref{table:graph-clustering-survey}
were selected to be utilized as part of our work --- they are discussed
in more detail in the \textit{\nameref{chap:background}} subsection regarding
\textit{\nameref{subsect:renowned-graph-clustering-algorithms}}.
The citation numbers in the table are also referenced there.

\begin{table}[ht!]
\def\arraystretch{1.60}
\begin{tabularx}{\textwidth}{|Y||p{2.5cm}|Y|Y|Y|Y|Y|Y|Y|Y|Y|}
 \hline
 \rot{Citations~} & \rot{Runtime complexity~} & \rot{Implementation available~} & \rot{Does not require amount of clusters~} & \rot{Deterministic~} & \rot{Considers weighted edges~} & \rot{Hard clustering~} & \rot{Hierarchical~} \\
 \hline\hline
 \cite{girvan2002community}     & \(\mathcal{O}(n^3)\)          & $\oplus$  & $\ominus$  & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{vandongen2000graph}      & \(\mathcal{O}(nk^2)\)         & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{pons2005computing}       & \(\mathcal{O}(mn)\)           & $\oplus$  & $\ominus$  & $\oplus$   & $\oplus$  & $\ominus$ & $\ominus$ \\\hline
 \cite{clauset2004modularity}   & \(\mathcal{O}(md \log{n})\)   & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{blondel2008modularity}   & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{rosvall2008infomap}      & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\ominus$  & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{biemann2006chinese}      & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\ominus$  & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{reichardt2004detecting}  & \(\mathcal{O}(n)\)            & $\oplus$  & $\ominus$  & $\ominus$  & $\oplus$  & $\ominus$ & $\ominus$ \\\hline
 \cite{donetti2004detecting}    & N/A                           & $\oplus$  & $\ominus$  & ~          & $\ominus$  & $\oplus$  & $\oplus$ \\\hline
\end{tabularx}
\caption{Graph clustering algorithm survey matrix}
\caption*{
  All of the information necessary for the classification including the
  runtime complexity indications are taken out of the original papers of the
  authors which leads to the fact that some information is unknown as
  the authors did not calculate or disclose this information.
  The signs that are used in the matrix correspond to the following meaning:
  \begin{itemize}[noitemsep]
    \item $\oplus$: The statement of the respective cell is \textit{true}
    \item $\ominus$: The statement of the respective cell is \textit{false}
    \item N/A: The statement of the respective cell is \textit{not available}
  \end{itemize}
  The way that classifications are negated was chosen so that $\oplus$
  is a positive feature regarding the utilization as part of our work.
  % The citation numbers stand for the following algorithms:
  % \begin{itemize}[noitemsep]
  %   \item \cite{girvan2002community}: Girvan-Newman
  %   \item \cite{vandongen2000graph}: MCL
  %   \item \cite{pons2005computing}: Walktrap
  %   \item \cite{clauset2004modularity}: Clauset-Newman-Moore
  %   \item \cite{blondel2008modularity}: Louvain
  %   \item \cite{rosvall2008infomap}: Infomap
  %   \item \cite{biemann2006chinese}: Chinese Whispers
  %   \item \cite{reichardt2004detecting}: Spinglass
  %   \item \cite{donetti2004detecting}: Commfind
  % \end{itemize}
}
\label{table:graph-clustering-survey}
\end{table}

The characteristics chosen for the survey are discussed in the following paragraphs.

\paragraph{Runtime complexity}
The runtime complexity of a graph clustering algorithm is not overly
important for our methodology and definitely does not eliminate a given algorithm,
nevertheless, it is interesting to include this metric in the conducted survey.

\paragraph{Implementation available}
This characteristic is the first rated one as it is the most important for our
methodology in the sense that the utilization of an algorithm as part of
the implementation of our methodology is only possible when an implementation
of said algorithm is available.

\paragraph{Does not require amount of clusters}
For our devised methodology, it is beneficial if the utilized graph clustering
algorithm does not require a desired amount of clusters a priori owing to
the fact that this number can only be estimated at best by a software engineer
using our implementation.
Nevertheless, some algorithms, and more importantly even some of the selected ones,
require said amount of clusters as an input parameter.
To circumvent this problem we came up with the solution to execute the affected
algorithms multiple times with the same input and differing cluster amounts
for a fixed number of iterations. Afterwards, the run that produced the highest
scoring metric is selected in place where other algorithms that are not affected
would have yielded one output in the first place.
This may sound like a very ineffective solution but due to the fact that
a single run of any graph clustering algorithm is completely isolated from
another one, this process can be run concurrently with ease.
How exactly this is implemented is discussed in chapter \ref{chap:implementation}.

\paragraph{Deterministic}
A non\hyp deterministic graph clustering algorithm might produce different
results over the course of multiple executions. This effect is not a desired
characteristic regarding our devised methodology but it can be mitigated
with the solution discussed in the previous paragraph again.

\paragraph{Considers weighted edges}
This characteristic is of great importance for us since the input graphs
will always be weighted. Furthermore, the whole idea of our methodology
is to cluster similar vertices where the similarity is resembled via the edge weights.

\paragraph{Hard clustering}
The term \textit{hard clustering} describes that the output clustering
of a given graph clustering algorithm assigns one and only one cluster
to each vertex of the graph. Comparatively, \textit{soft clustering} or
\textit{fuzzy clustering} can assign a vertex to multiple clusters which
produces overlapping clusters \cite{lancichinetti2009community}.
Similarly to the previous characteristic, this is not desired regarding our
devised methodology as every vertex can only belong to one microservice
which requires \textit{hard clustering} (i.e. a partitioning).

\paragraph{Hierarchical}
The last characteristic is not directly important for our methodology
as we are only interested in the final output of the clusters as partitions.
Nevertheless, as discussed in section \ref{sect:future-work},
this property of graph clustering algorithms could be leveraged for future features.
\newline

The reason that the first seven from table \ref{table:graph-clustering-survey}
were selected is that specifically the characteristics
\textit{Considers weighted edges} and \textit{Hard clustering} are mandatory
for our devised methodology to function.



\section{Clustering the combined weighted graph}

Regardless of which of the first nine graph clustering algorithms out of
table \ref{table:graph-clustering-survey} is chosen, the input and output
is the same in its core.
Although the exact input and output formats differ between the algorithms,
it always boils down to passing a list of edges to the algorithm ---
listing \ref{graph-clustering-algorithm-input} depicts this kind of input format.
In this example input, the format follows a tuple of three items (i.e. a triple),
namely a vertex, another vertex it is connected to, and the edge weight of the
edge connecting them:
\[
  I_i = (v_1, \ v_2, \ W(e_{(1,2)}))
\]
such that \(v_1, v_2 \in V, \ e_{(1,2)} \in E, \ G = (V, \ E)\) while
\(e_{(1,2)}\) is the edge connecting the vertices \(v_1\) and \(v_2\).

\begin{lstlisting}[caption=Example graph clustering algorithm input, label=graph-clustering-algorithm-input, breaklines=true]
1 2 0.50
4 5 0.11
5 6 0.55
6 5 0.60
3 2 0.19
\end{lstlisting}

In this format, the entire input graph is serialized while each line resembles
a single edge. The integers used for the vertices are their identifiers,
some algorithms require identifiers from \([1, n] \in \mathbb{Z}\) where \(n\)
is the number of edges, others also work with arbitrary strings as identifiers.
This fact is discussed in more detail in chapter \ref{chap:implementation}.

After serializing the input graph, the chosen algorithm clusters the graph
and returns the clustering as its output. The inner workings of the chosen
algorithm are assumed as a black-box function at this point since this was
already discussed in subsection \ref{subsect:renowned-graph-clustering-algorithms}
and as it is not important for the understanding of this current section.

Similarly to the input, the output of the selected algorithms also shares
a common notion regarding the format. This example output format is depicted
in listing \ref{graph-clustering-algorithm-output}.
In contrast to the input format, the output contains vertices compared to edges
and the cluster identifier of each vertex compared to edge weights,
as pairs this time:
\[
  O_i = (v_i, \ c_i)
\]
such that \(v_i \in V, \ G = (V, \ E)\) while \(c_i \in C\) resembles the
cluster that was assigned to the respective vertex by the graph clustering
algorithm. The cluster identifiers commonly range from \([1, n] \in \mathbb{Z}\)
where \(n\) is the amount of clusters. Moreover, they can be duplicated
which signifies that vertices were put into the same cluster.

\begin{lstlisting}[caption=Example graph clustering algorithm output, label=graph-clustering-algorithm-output, breaklines=true]
1 1
2 1
3 1
4 2
5 3
6 3
\end{lstlisting}

Finally, we deserialize the output into a Java object so it can be used to
calculate metrics and render the clustered weighted graph for the user.




\chapter{Calculating metrics on microservice recommendations} \label{chap:metrics}





\section{Input fidelity}





\section{General clustering quality}





\section{Coupling modularity}






\chapter{Implementation} \label{chap:implementation}

\section{Database} \label{sect:implementation-database}

\section{Back-end} \label{sect:implementation-back-end}

\section{Front-end} \label{sect:implementation-front-end}

\subsection{Visualizing microservice recommendations}

\subsubsection{Network view}

\subsubsection{Textual view}

\subsubsection{Metrics view}






\chapter{Results} \label{chap:results}

\section{Experimental setup}

\section{Evaluation}




\chapter{Discussion} \label{chap:discussion}

\section{Threats to validity} \label{sect:threats-to-validity}

\section{Future work} \label{sect:future-work}

\section{Conclusion} \label{sect:conclusion}






% Reference lists
\newpage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures
\newpage
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables
\newpage
\addcontentsline{toc}{chapter}{Bibliography}
% Separate the sources with 'bibtopic'
\bibliographystyle{plain}
\begin{btSect}{references}
\section*{\huge{References}}
\btPrintCited
\end{btSect}
\begin{btSect}{online}
\section*{\huge{Online Sources}}
\btPrintCited
\end{btSect}

\end{document}
