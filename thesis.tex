\documentclass[12pt,a4paper]{report}

\include{settings}

\newcommand{\studyprogramme}{Software Engineering}
\newcommand{\degreetype}{Master of Science}
\newcommand{\thesistitle}{
Toward systematic decomposition of monolithic software into microservices
}
\newcommand{\thesissubtitle}{
SUBTITLE
}
\newcommand{\thesisauthor}{Jakob L{\"o}hnertz}
\newcommand{\thesisdate}{31/08/2019}
\newcommand{\thesislocation}{Amsterdam}
\newcommand{\firstmarker}{Dr.\ Ana Oprescu}
\newcommand{\secondmarker}{MSc.\ Stephan Schroevers}
\newcommand{\hostorganization}{Picnic B.V.}

\begin{document}



\include{preamble}



\begin{abstract}

ABSTRACT

\end{abstract}



\tableofcontents



\chapter{Introduction} \label{chap:introduction}

Software engineering develops faster than ever, the amount of novelties in
platforms, languages, operational strategies and generally the terminology
around it gets larger and larger every day.
One of the terms that gained a lot of traction in recent years is \textit{microservices}.
Many engineers talk about their benefits and challenges but the general notion
is positive and the buzz is undeniable.
At the end of the day, microservices are nothing more than an architectural style
for designing back-end software.
For those that actually take the step to build and deploy such an application
landscape, the journey is not always straightforward.
The idea is still evolving and mandates many new strategies and concepts
that developers and operations have to deal with
when choosing to devote their business logic to leverage microservices.

There are many good reasons to employ this pattern nowadays specifically
when dealing with ever larger growing monolithic applications that try to
encapsulate their respective business logic without drowning the
developers and engineers working on it in seemingly endless complexity
that a single human cannot even fully comprehend anymore at one point.

For many companies, this oftentimes is the status quo and if the decision is
made to move to a microservices-based architecture, one of the first
challenges, the decomposition, is already difficult to begin with:

Defining where within the existing application one service should end and the
next one should begin --- i.e. detecting latent boundaries that partition
the monolith into many smaller pieces, is a very manual and tedious task.

Thus, this thesis develops an algorithmic methodology that can assist
a human software engineer in performing the aforementioned decomposition.



\section{Problem analysis}

Generally speaking, there are two ways to put a microservices-based
software architecture into place. The greenfield approach, meaning that
microservices are directly getting built without a preexisting application,
as well as the transformation of an existing monolithic software solution
into independent microservices \cite{fowler-break-monolith}.
However, this thesis focuses solely on the latter approach for a variety of reasons.

Firstly, our work leverages static and dynamic program analysis for solving
the posed task. Obviously, this is not possible without an already running solution.

Secondly, while there is already some research on this topic \cite{fritzsch2018monolith},
which is discussed in the \textit{\nameref{chap:related}} chapter, the notion
of utilizing the already existing and running application is still underrepresented.

Thirdly, as Martin Fowler remarks, microservices are especially feasible
with a mature workflow, domain model, operational model, etc. already
in place \cite{fowler-monolith-first, fowler-microservices-tradeoffs}
which favors non-greenfield approaches as well.

The task of getting the boundaries of each microservice right is hard,
especially when the teams working on the existing software
are caught up in their mental models of it \cite{latoza2006maintaining},
as Fritzsch et al. confirm:
\begin{displayquote}
\emph{"Extracting a domain model from an application's code base
can be a significant challenge. If incorrectly applied,
it can lead to architectures that combine the drawbacks of both styles,
monolithic architectures and Microservices."}~\cite{fritzsch2018monolith}
\end{displayquote}
To prevent such an outcome, we propose a semi-automated approach to assist
software engineers in decomposing microservices out of an existing monolithic code base.
This approach has the advantage that the mental models regarding the existing
solution might be broken apart partially to offer new perspectives onto how
the current software works, which was already remarked as a positive side effect
of an assisted microservices decomposition approach \cite{gysel2016service}.
Thus, the engineers and architects can be assisted in the first step,
which is likely also the most difficult and time-consuming one due to the possible
complexity of the system \cite{fritzsch2018monolith, france2007model},
when undertaking the operation of transforming their existing
monolithic solution into microservices.

Additionally, we are certain that the actual decisions should still be made
by seasoned engineers and architects which is why a semi-automated solution
was determined as the best option to support them in their
decision making while not taking the decision making away from them
(i.e. compared to an automated decomposition approach).
As a consequence, everything mentioned in this section reflects itself in
the following research questions of this thesis.



\section{Research questions}

\hangindent=1cm
\noindent Thus, we derived the following research questions:\\
\textbf{RQ1:} \textit{"How can existing monolithic software be analyzed
and used to give microservices recommendations based on it?"}\\
\textbf{RQ2:} \textit{"Which numeric metrics are the most suitable
for evaluating microservice decomposition methodologies?"}



\section{Research methods} % TODO: Validate this section

Firstly, the approach for the methodology is developed and validated
for feasibility especially regarding its implementation.
Apart from its general description in the \nameref{chap:related} chapter,
the approach itself is explained and devised in chapters \ref{} to \ref{}.

Secondly, the foundation for the rationale behind the methodology is worked out
as the approach itself is abstracted away from the topic of
software decomposition and instead built upon graph theory.
Precisely, this encompasses a mapping of the inputs into the methodology to
their graph theoretic representation as well as the final output.
Otherwise, no logic reasoning is possible to tie the results and
their calculated metrics back to the inputs and the underlying subject
of decomposing monolithic software into microservices in the first place.
This part of the research is covered in chapter \ref{chap:rationale}.

Likewise, not only a theoretical methodology but also an implemented
proof-of-concept (PoC) are the outputs of this project.
The latter is introduced in chapter \ref{}.
The steps that this PoC needs to conduct are derived and tested
independently via exploratory as well as confirmatory case studies
in chapter \ref{} to receive substantive, numeric,
and separate results \cite{easterbrook2008selecting}.
Moreover, controlled experiments are not feasible as control results
with their dependent variables for the outcomes are rare and difficult
to find and measure \cite{easterbrook2008selecting}.

Finally, the research is validated with real-world monolithic applications.
A large-scale code base provided by the company hosting this thesis as well as
large open source software projects were candidates for this.
The evaluation as part of chapter \ref{} is conducted using
a custom set of metrics devised in chapter \ref{}.




\chapter{Background} \label{chap:background}

Three domains are mainly important for grasping the entirety of this work.
General computer science knowledge is expected but the following
three subjects are covered in more detail on top of it:
\textit{\nameref{sect:background-architecture}},
\textit{\nameref{sect:background-program-analysis}}, and
\textit{\nameref{sect:background-graph-clustering}}.



\section{Software architectures} \label{sect:background-architecture}

As seen in the \textit{\nameref{chap:introduction}}, our work revolves around
the decomposition of monolithic software into microservices.
Thus, the following subsections focus on these two architectural styles for
building back-end applications.


\subsection{Monolithic}
The term \textit{monolithic} is derived from the Ancient Greek word
\textit{mon√≥lithos} which roughly translates to \textit{single stone} \cite{press2011oxford}.
This adumbrates already what the word is used for in software engineering,
where it, specifically regarding enterprise back-end applications,
describes software that is built, deployed, and executed
as a single logical unit \cite{ms-fowler}.
Furthermore, a monolithic application can perform every task of the given functionality
within the domain of its business logic from beginning to end \cite{monolith}.
Usually this is implemented in a layered (e.g. UI, business logic, data access)
and modular manner \cite{ms-fowler, monolith}.


\subsubsection{Advantages}
Generally, the notion exists that monolithic applications are more natural
to work with while also being easier to design, build, and deploy specifically
in the beginning of their life cycle \cite{ms-fowler, raymond2003unix}.

Tying into this, the operational overhead is small as not much middleware or
special deployment strategies are necessary \cite{ms-fowler}.
This becomes specifically apparent when compared to microservices
in the following subsection.

Finally, the performance is usually better compared to architectures dependent
on inter-process communication \cite{knoche2016sustaining}.


\subsubsection{Disadvantages}
Even if the monolithic application is built modular and in layers,
the code base and most likely the complexity can just grow over time.
The engineer is in the end just fighting against it becoming a
\textit{big ball of mud} which starts to exceed the mental capabilities of a
human engineer \cite{newman2015building, foote1997bigballofmud, france2007model}.

Moreover, a monolithic application is deployed as a whole which hinders
independent life cycles regarding the modules within the application \cite{ms-fowler}.
Specifically in agile software development structures, this can cause immense
conflicts when trying to release new versions of the application.

Finally, following the last disadvantage, the scalability is also problematic
as the only option once the application needs to be scaled is to replicate it
into another instance and balance incoming load onto the instances ---
even if only a tiny module within the monolith might have needed any scaling
in the first place \cite{ms-fowler, newman2015building}.


\subsection{Microservices-based}
Microservices are essentially about building an application
\textit{"as a suite of small services, each running in their own process and
communicating with lightweight mechanisms, often an HTTP resource API"} \cite{ms-fowler} ---
a microservice is developed and deployed independently in a distributed manner.
Each service should be built around one business capability following the
\textit{single responsibility principle} \cite{newman2015building, martin2003agile}.
Furthermore, each microservice should be responsible for the data it governs,
avoiding any overlaps with other services \cite{ms-fowler}.


\subsubsection{Advantages}
As just mentioned, microservices-based architectures allow for a better
decoupling of software development life cycles between business capabilities \cite{ms-fowler}.
This enables different teams or even single developers to work at different
velocities which is sometimes also dictated by the type and complexity of the
developed functionality.

Additionally, once the foundation of the architectural design is established,
it is a lot more effortless to keep functionalities decoupled as
they are not even part of the same process once running.

Moreover, it is easier to incorporate multiple programming languages into
the application landscape that fit the tasks the best.
In the end, each service just offers an external API for the others to use
and consume which is agnostic of programming language as long as it adheres
to the contract provided by the interface specification.

Finally, the scalability of microservices is far better compared to monolithic
applications. When leveraging features of cloud computing, just services
that are in need of scaling can be scaled instead of the entirety of the application.


\subsubsection{Disadvantages}
In contrast to monolithic applications, microservices introduce a non-negligible
amount of overhead. Firstly, the architectural style is geared toward
agile software development which by itself mandates a certain organizational setup.
Secondly, the development demands other patterns in terms of e.g.
error-handling and data management compared to a monolithic software design.
Lastly, the deployment is more fine-grained and usually encompasses a lot of
cloud computing facilities such as \textit{Infrastructure as a Service} (IaaS)
together with certain topics completely unheard of in monolithic architectures
such as \textit{service discovery} which is dealing with the task of each
service knowing where and how to reach the others \cite{newman2015building}.

Furthermore, the penalty on inter-service communication via a network is almost
always larger compared to regular function calls, especially when considering
serialization and deserialization before and after every inter-service call
\cite{knoche2016sustaining, newman2015building}.

Additionally, designing microservices, similarly to a modular monolithic design,
is a grand task. However, the boundaries need to be defined even more strictly
as migrating certain functionality and data later on has a bigger impact than
doing the same in a monolithic application, since data can oftentimes
stay where it is in a monolithic architecture.
Likewise, the interfaces between services need more care than interfaces in a
monolithic application due to their distributed nature that can lead to
performance and fail-over problems a lot faster as a network is acting
in between.
Finally, Conway's law \cite{conway1968law} is more important than ever
in a microservices landscape as overly dependent life cycles between services
will lead to nothing more than a \textit{distributed monolith} in the long run.
\newline


In summary, the larger and more complicated the underlying business logic
becomes, the more the application can benefit from a microservices-based architecture.
Specifically when developing the overarching application in agile teams,
microservices offer natural boundaries for this organizational structure.
Furthermore, when looking at the disadvantages of them, the question of where
to draw the boundaries between microservices is especially difficult,
yet it is what this thesis revolves around as it can benefit from
algorithmic assistance to the work of human software engineers a lot.



\section{Software analysis} \label{sect:background-program-analysis}

As no further user input should be required by our methodology,
it has to gather information out of inputs on its own.
Thus, various types of program analysis are leveraged to
algorithmically collect data that can be used as the input for the methodology.


\subsection{Static program analysis}
The term \textit{static program analysis} is best defined by comparing it
to \textit{dynamic program analysis} --- instead of monitoring the program
as it is running, the code, in many possible facets, is analyzed without
actually executing it. While static program analysis has many categories
\cite{woegerer2005static}, the only two sub-categories relevant to this work
are \textit{call graphs} and \textit{semantic analysis} while the
latter has its roots in \textit{natural language processing} especially in
the way that it is utilized in our methodology.


\subsubsection{Call graph generation} \label{subsubsect:call-graph}
Generally, a call graph is consisting of \textit{"nodes that are the routines
of the program and directed arcs that represent calls from call sites to
routines"} \cite{graham1982gprof}. Those graphs can be created via static
as well as dynamic program analysis, although the latter is covered in
subsection \ref{subsect:background-dynamic-analysis}.
In an object-oriented program, the routines would represent methods
or even whole classes \cite{grove1997callgraph}.

The two basic ways to represent the weight \(W(e_i) \in \mathbb{R}\)
of a weighted edge \(e_i \in E\) as part of a call graph \(G = (V, E)\)
are \textit{execution counts} and \textit{execution times} although
\textit{execution times} are only measurable via
dynamic program analysis \cite{graham1982gprof}.

However, in our approach, the static program analysis that we employ has
auxiliary purposes since other types of program analysis, particularly
dynamic program analysis, will seldom yield a complete graph theoretic
representation of the analyzed program \cite{graham1982gprof}, such that
each routine from the input program \(r_i \in R\) is part of the output graph
\(G = (V, E)\) as a vertex \(\forall r_i \in V\) while each \textit{execution}
\(x_i \in X\) is an edge \(\forall x_i \in E\) connecting
two such vertices \(x_i = (r_1, r_2), \enskip r_1 \in V, \enskip r_2 \in V\).

We utilize the static program analysis without \textit{execution counts}
which does not yield a weighted graph, allows us however to have a more complete
weighted call graph when merging two or more incomplete call graphs, weighted or not,
e.g. when another was constructed with dynamic program analysis.
Precisely, this is discussed in the chapter \nameref{}.


\subsubsection{Semantic similarity calculation}
Calculating the semantic similarity between two documents is a traditional
discipline in the domain of natural language processing (NLP).
The similarity value usually ranges from \([0.0, 1.0]\) ---
a percentage \cite{singhal2001ir}.

As anthropogenic source code is largely consisting of natural language
e.g. in the form of identifiers, variable names, and comments, it can be
semantically analyzed as well.

The basic steps to calculate semantic similarity between documents in
any given corpora (a \textit{corpus} is a collection of \textit{documents})
as a mathematical process are \cite{singhal2001ir}:
\begin{enumerate}
  \item Tokenization
  \item Stop word filtering
  \item Normalization
  \item Vectorization
  \item (Optional) Relevance weighting
  \item (Optional) Latent semantic indexing (LSI)
\end{enumerate}

\paragraph{Similarity calculation}
All of the abovementioned steps have to be performed for every document
in the corpus until the resulting vectors can be run through a function that
calculates their similarity. A common approach for this is the
\textit{cosine similarity} which calculates the cosine of an angle \(\theta\)
between two input document vectors that are part of the corpus
\(\{\vec{\mathbf d_1}, \vec{\mathbf d_2}\} \in C\) \cite{singhal2001ir}:
\[
  \cos(\theta) = \frac{
    \vec{\mathbf d_1} \cdot \vec{\mathbf d_2}
  }{
    \|\vec{\mathbf d_1} \|\|\vec{\mathbf d_2} \|
  }
\]

\paragraph{Tokenization}
The goal of the first step is to retrieve a list of terms (i.e. words)
of a given document in the corpus \(\forall t_i \in D_i, \enskip D_i \in C\).
Importantly, each document \(D_i\) is a sequence not a set
(i.e. is allows the repetition of elements), otherwise the 5th step would not
work as shown later.
Consequently, punctuation characters are stripped from the document and
the text get split for instance on spaces \cite{singhal2001ir}.

\paragraph{Stop word filtering}
Language naturally contains terms that do not carry much information,
those are called \textit{stop words} \cite{singhal2001ir}.
Common examples from the English language are words such as:
\textit{the}, \textit{and}, \textit{or} etc.
The useful semantics can be found in the words that are left after filtering
out such \textit{stop words}.

\paragraph{Normalization}
Both of these terms refer to the process of removing the endings of all terms
in a given document \(\{t_i \in D_i \mid normalization(t_i)\}\) that is part of
the corpus \(D_i \in C\) \cite{singhal2001ir}.

The rationale of this step is to normalize the semantics of the terms ---
\(\{\textit{organization}, \textit{organizer}, \textit{organizing}\}\) essentially
encapsulate the same semantic meaning of their \textit{lemma}: \textit{organize}.
Nevertheless, if not normalized, a purely mathematical approach,
which is utilized here, will not capture the common denominator of those terms.
Consequently, each term is stripped of its ending. There are two major approaches
to do so: \textit{lemmatization} and \textit{stemming} \cite{plisson2004lemma}.

The \textit{lemma} of a word is its \textit{canonical form}, its base form \cite{plisson2004lemma}.
Thus, \textit{lemmatization} describes the process of normalizing words to their
\textit{lemmata} \cite{plisson2004lemma}.

The other variant, the \textit{stemming}, describes a similar process with the
same goal that instead just strips common endings in the respective language.
Therefore, \textit{lemmatization} is objectively the better, albeit more complex,
approach as it captures the actual base form of a word while also
being less error-prone. A good example are the English words:
\(\{\textit{runs}, \textit{running}, \textit{ran}\}\). It becomes apparent that
in this example, the \textit{stemming} approach would not have correctly
normalized the word \textit{ran}, compared to the \textit{lemmatization} approach.

Ergo, if possible, \textit{lemmatization} is preferred over \textit{stemming}.

\paragraph{Vectorization}
Since the described process is mathematical in its core, the sequence of terms
out of each document of the corpus needs to be converted to
numerical representations of themselves.
Precisely, this step involves constructing a term-document matrix
with the all terms contained in the corpus \(\forall t_i \in \bigcup C\)
as the rows and all the documents in the corpus \(\forall D_i \in C\)
as the columns. It is filled with either a \(0\) if a term of a given row
does not appear in the document of that column or a \(1\) if it does appear:
\[
  \begin{matrix}
    & \textbf d_i \\
    & \downarrow \\
    \textbf t_i^{T} \rightarrow &
    \begin{bmatrix}
      td_{1,1}  & \dots   & 1       & \dots   & td_{1,n}  \\
      \vdots    & \ddots  & \vdots  & \ddots  & \vdots    \\
      1         & \dots   & 0       & \dots   & 0         \\
      \vdots    & \ddots  & \vdots  & \ddots  & \vdots    \\
      td_{m,1}  & \dots   & 0       & \dots   & td_{m,n}  \\
    \end{bmatrix}
  \end{matrix}
\]
Consequently, this results in a document vector
\(\vec{\mathbf d_i}\) for every column \(td_{*,n}\)
of the matrix, which is necessary for the remaining steps.

\paragraph{Relevance weighting}
Although the resulting document vectors could already be used for the
\textit{cosine similarity} calculation, the quality of the output
can still be improved by weighting every term to increase or decrease its value
in the matrix per document based on its relevance for that document in relation
to the whole corpus.

A common approach, that we are utilizing,
is \textit{term frequency‚Äìinverse document frequency} (\textit{tf-idf}).
The idea behind it is to increase the weight of terms that have
a high \textit{term frequency} and a low \textit{inverse document frequency}.
This tends to filter out common terms while overall improving the relevance
of the terms in the corpus \cite{robertson2004tfidf, singhal2001ir}.
Theoretically, one can skip the step of \textit{stop word filtering}
as \textit{tf-idf} in the end achieves a similar goal by not returning
high values for common terms that might have been considered \textit{stop words}.

It first independently calculates the \textit{term frequency} for each term,
such that the frequency of a term in a given document is divided by the total
amount of terms in that document \cite{robertson2004tfidf}.
The goal is to normalize the importance of a term owing to the fact
that longer documents have a higher chance of containing the given term
more often \cite{singhal2001ir}:
\[tf(t,d) = \frac{f_{t, d}}{\vert d \vert}\]

Next, the \textit{inverse document frequency} is calculated for each term,
such that the logarithm of the total amount of documents in the corpus
is divided by the amount of documents containing the term \cite{robertson2004tfidf}.
This calculation quantifies the importance of a term toward the corpus \cite{singhal2001ir}:
\[idf(t,C) = \log \left(\frac{\vert C \vert}{\vert \{d \in C \mid t \in d\} \vert}\right)\]

Finally, the functions are combined to calculate the \textit{tf-idf} value
of each term on a per-document basis \cite{robertson2004tfidf}:
\[tfidf(t,d,C) = tf(t,d) \times idf(t,C)\]

Applying \textit{tf-idf} changes the values of the elements in the matrix
from \(\forall td_{m,n} \in \{td \in \mathbb{Z} \mid td \le 0\}\)
to \(\forall td_{m,n} \in \{td \in \mathbb{R} \mid td \le 0\}\) due to
the two divisions involved.

\paragraph{Latent semantic indexing (LSI)}
This last step is optional but oftentimes utilized to further improve the quality
of the output of this process. \textit{Latent semantic indexing} (LSI) describes
the process of applying \textit{singular value decomposition} (SVD) to
the aforementioned term-document matrix \cite{deerwester1990lsi}. % TODO: Should one go into more detail?
The rationale in the context of the similarity calculation is
to reduce the matrix with \(m\) rows and \(n\) columns to one with a
set amount of \(z\) dimensions as the rows.
This is supposed to get reduce the effects of \textit{synonymy} and
\textit{polysemy} \cite{deerwester1990lsi}.
Precisely, \textit{synonymy} describes the phenomenon where different terms describe
the same idea \cite{press2011oxford} whereas \textit{polysemy} describes the phenomenon
where same term describes multiple ideas \cite{press2011oxford}.
After applying \textit{LSI}, each document vector of the corpus \(\vec{\mathbf d_i} \in C\)
has a magnitude of the previously chosen \(z\) amount of dimensions \(\|\vec{\mathbf d_i}\|\).
Consequently, the abovementioned \textit{cosine similarity} can be calculated.

\paragraph{Source code processing}
The main difference when processing natural language as part of source code
occurs in the first step, the \textit{tokenization}.
Truly natural language does not use many special characters such as
braces, semicolons, or ampersands which conversely see heavy usage in
many programming languages, especially ones with a C-esque syntax.
As a consequence, the \textit{tokenization} is specific to the respective
programming language that is getting processed as different languages use a
different set of control characters together with differing strategies
for concatenating identifiers and variable names consisting of
multiple words (e.g. camel case or snake case).

Furthermore, the \textit{stop word filtering}, if employed, has to be tailored
even more to the programming language getting processed ---
keywords (e.g. \code{public}, \code{static}, \code{void}) appear in
every source code document while not adding any meaningful semantic information,
hence it is useful to filter those out as part of the \textit{stop word filtering}.


\subsection{Dynamic program analysis} \label{subsect:background-dynamic-analysis}

Next to \textit{static program analysis}, one can also analyze the behavior of
an application while it is running, this is called
\textit{dynamic program analysis} \cite{ernst2003static}.
The reasons to utilize \textit{dynamic program analysis} are manifold,
to gather information about runtime performance, memory usage,
or debugging \cite{ernst2003static}.
Another application is the creation of a call graph as discussed in subsection
\ref{subsubsect:call-graph}. For the same reasons and a few others on top,
this the process of generating a \textit{dynamic call graph} are interested to our work.


\subsubsection{Call graph generation}
Additionally to the aforementioned reasons to create a \textit{static call graph},
a \textit{dynamic call graph} can provide other useful information.
Precisely, to discover where the most data flows within the monitored application
is a main application of this process \cite{graham1982gprof}.
The common approach is to build a weighted and optionally directed graph
to represent the call graph. The optional directions as the directions the
routines invoke each other and e.g. \textit{execution counts} or average
\textit{execution times} as the weighted edges \cite{graham1982gprof}.
Specifically the latter can only be recorded during the runtime of the application,
however for our work only \textit{execution counts} are used as the edge weights.
The general problem with \textit{dynamic program analysis} however is that one
firstly has to run the application that is to be analyzed and secondly the fact
that there is generally no guarantee that even every part of the application
has been invoked even when analyzed for an immense amount of time which will
then lead to an incomplete \textit{dynamic call graph} \cite{graham1982gprof}.



\section{Graph clustering} \label{sect:background-graph-clustering}

The term \textit{graph clustering}, sometimes also called \textit{community detection},
describes the process of partitioning a usually undirected
and unweighted or weighted graph into subgraphs \cite{lancichinetti2009community}.
Important to our work is that partitions on the input graph are calculated,
so that the resulting clusters do not overlap although a subset of
graph clustering algorithms exists that generates overlapping clusters.
A cluster is typically described as a group of vertices that are densely
interconnected via their \textit{intra-cluster edges} while being only sparsely
connected to other vertices outside of their own cluster via
\textit{inter-cluster edges} \cite{lancichinetti2009community, newman2004fast}.

\paragraph{Modularity}
The metric of \textit{modularity} measures the quality of a graph clustering
by comparing the interconnectedness of intra-cluster vertices to their
inter-cluster edges. It works on unweighted as well as weighted graphs
\cite{clauset2004modularity, blondel2008modularity}
and was devised in 2004 by Newman \cite{newman2004fast}:
\[
  Q =
  \frac{1}{2m}
  \sum \limits _{ij}{\bigg[ A_{ij} - \frac{k_i k_j}{2m} \bigg]}
  \delta (c_i, c_j)
\]
where
\begin{itemize}[noitemsep]
  \item \(A_{ij}\) is the edge weight between vertices \(i\) and \(j\),
  \item \(k_i\) and \(k_j\) is the sum of the edge weights attached to
        the vertices \(i\) and \(j\),
  \item \(2m\) is the sum of all edge weights,
  \item \(c_i\) and \(c_j\) represent the clusters of the vertices \(i\) and \(j\),
  \item \(\delta\) is a delta function
\end{itemize}
The metric ranges on a scale from \([0.0, 1.0]\), from no clustering structure
at all to a perfect clustering. Any given graph has a theoretic maximum in terms
of the achievable \textit{modularity}. However, it is rarely \(1.0\) and finding
the global maximum is proven to be an NP-complete optimization problem
\cite{brandes2006maximizing}. Nevertheless, the notion in literature is that
values over \(0.4\) already resemble a strong clustering structure
\cite{newman2004fast, fortunato2007resolution}.


\subsection{Renowned algorithms}

A variety of graph clustering algorithms exists with a variety of different
goals, mechanisms, and features
\cite{lancichinetti2009community, fortunato2010community, danon2005comparing}.
Consequently, a dozen renowned graph clustering algorithms were evaluated and
assessed regarding their usability for our work.
Eventually, the first seven algorithms from table \ref{table:graph-clustering-survey}
were selected to be utilized as part of our work --- they are discussed
in more detail below and are additionally marked with their
reference to identify them in table \ref{table:graph-clustering-survey}.
Finally, the way that we utilize the exhibited graph clustering algorithms
as part of this work is discussed further in chapter
\ref{chap:creating-service-recommendations}.

\begin{table}[h!]
\def\arraystretch{1.60}
\begin{tabularx}{\textwidth}{|Y||p{2.5cm}|Y|Y|Y|Y|Y|Y|Y|Y|Y|}
 \hline
 \rot{Citations~} & \rot{Runtime complexity~} & \rot{Implementation available~} & \rot{Does not require amount of clusters~} & \rot{Deterministic~} & \rot{Considers weighted edges~} & \rot{Hard clustering~} & \rot{Hierarchical~} \\
 \hline\hline
 \cite{girvan2002community}     & \(\mathcal{O}(n^3)\)          & $\oplus$  & $\ominus$  & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{vandongen2000graph}      & \(\mathcal{O}(nk^2)\)         & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{pons2005computing}       & \(\mathcal{O}(mn)\)           & $\oplus$  & $\ominus$  & $\oplus$   & $\oplus$  & $\ominus$ & $\ominus$ \\\hline
 \cite{clauset2004modularity}   & \(\mathcal{O}(md \log{} n)\)  & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{blondel2008modularity}   & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\oplus$   & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{rosvall2008infomap}      & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\ominus$  & $\oplus$  & $\oplus$  & $\oplus$ \\\hline
 \cite{biemann2006chinese}      & \(\mathcal{O}(n)\)            & $\oplus$  & $\oplus$   & $\ominus$  & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{reichardt2004detecting}  & \(\mathcal{O}(n)\)            & $\oplus$  & $\ominus$  & $\ominus$  & $\oplus$  & $\ominus$ & $\ominus$ \\\hline
 \cite{donetti2004detecting}    & N/A                           & $\oplus$  & $\ominus$  & ~          & $\ominus$  & $\oplus$  & $\oplus$ \\\hline
\end{tabularx}
\caption{Graph clustering algorithm survey matrix}
\label{table:graph-clustering-survey}
\end{table}

\paragraph{Graph clustering algorithm survey}
A total of nine renowned graph clustering algorithms were evaluated
and classified in table \ref{table:graph-clustering-survey}.
All of the information necessary for the classification including the
runtime complexity indications are taken out of the original papers of the
authors which leads to the fact that some information is unknown as
the authors did not calculate or disclose this information.
The signs that are used in the matrix correspond to the following meaning:
\begin{itemize}[noitemsep]
  \item $\oplus$: The statement of the respective cell is \textit{true}
  \item $\ominus$: The statement of the respective cell is \textit{false}
  \item N/A: The statement of the respective cell is \textit{not available}
\end{itemize}
The way that classifications are negated or not was chosen so that $\oplus$
is a positive feature regarding the utilization as part of our work.

\paragraph{Girvan-Newman \cite{girvan2002community}}
The \textit{Girvan-Newman} algorithm is seen as one of the earliest, successful,
wide-spread graph clustering algorithm \cite{lancichinetti2009community}.
It was released by Girvan and Newman as part of their influential 2002 paper:
\textit{"Community structure in social and biological networks"} \cite{girvan2002community}.
Regardless of the title, their algorithm is generic and works on any graph network
as its input \cite{lancichinetti2009community}.
Their algorithm is performing hierarchical clustering which means that
every cluster that it creates can have recursive sub-clusters
again \cite{girvan2002community}.
Thus, this can alternatively be represented as a dendrogram \cite{newman2004fast}.

It iteratively removes edges with the highest
\textit{edge betweenness centrality}, which is based on
\textit{betweenness centrality} defined in 1977 by Freeman as
a metric that can be calculated for each vertex based on the idea that every
two vertices of a given graph have a shortest path that connects them,
the amount of how many of all those shortest paths go through any given vertex
represents its \textit{betweenness centrality} \cite{freeman1977set}.
As a consequence, Girvan and Newman adapted the idea for edges and coined it
\textit{edge betweenness centrality} \cite{girvan2002community}:
\[
  c(e) = \sum_{s, t \in V} \frac{\sigma(s, t \mid e)}{\sigma(s, t)}
\]
Here, \(\sigma(s, t \mid e)\) resembles the amount of shortest paths
between \(s\) and \(t\) that pass through \(e\) while \(\sigma(s, t)\)
is the amount of shortest paths between \(s\) and \(t\).

However, owing to the fact that their algorithm works hierarchically,
in the original implementation, the desired number of clusters has to be set
beforehand which causes the algorithm to cut edges until the set amount of
clusters is reached.

\paragraph{MCL \cite{vandongen2000graph}}
Two years before Girvan and Newman published their work, Van Dongen devised
the graph clustering algorithm \textit{MCL} \cite{vandongen2000graph}.
It works via random walks based on Markov chains by alternating two operations
on the input graph until it reaches a state of convergence regarding its
clustering structure \cite{vandongen2000graph}. These operations are
\textit{expansion} and \textit{inflation} \cite{vandongen2000graph}.

The first step of \textit{expansion} computes new probabilities for random walks
for every vertex pair in the graph. Owning to the fact that random walks within
clusters are usually of higher length as there are many edges to take, intra-cluster
vertex pairs usually receive higher probabilities \cite{vandongen2000graph}.

The second step, the \textit{inflation}, then boosts and demotes probabilities
further which over the course of many iterations will cause inter-cluster walks
to \textit{dry out} --- Van Dongen uses the terminology of \textit{flow} in fact
\cite{vandongen2000graph}. Consequently, this process will form a clustering
over time.

The strength of the \textit{inflation} can be set via an input parameter which
affects the granularity of the clustering and has therefore a similar problem
as the \textit{Girvan-Newman} algorithm. A solution to this issue is discussed
in chapter \ref{chap:creating-service-recommendations}.

\paragraph{Walktrap \cite{pons2005computing}}
The \textit{Walktrap} algorithm was devised in 2005 by Pons and Latapy
\cite{pons2005computing} and shares similarities with \textit{MCL}.
As its name implies, it tries to compute a graph clustering by utilizing short
Markov chain random walks and the idea that due to existing, albeit latent
clustering structure of a given graph, those random walks are more likely
to get \textit{trapped} in said clusters \cite{pons2005computing}.
%TODO: One could describe a little more here, maybe?!
The main improvement over \textit{MCL} is better runtime complexity
\cite{pons2005computing}.

\paragraph{Clauset-Newman-Moore \cite{clauset2004modularity}}
Two years after releasing the \textit{Girvan-Newman} algorithm, Newman
released another graph clustering algorithm \cite{newman2004fast}.
Its idea is based on maximizing the \textit{modularity} of the clustering.
However, shortly after, another improved version of his new algorithm
was published together with Clauset and Moore which is the
faster and generally improved version \cite{clauset2004modularity}.

As abovementioned, maximizing \textit{modularity} is an NP-complete
optimization problem which led Clauset et al. to solve it via a greedy heuristic.
Their algorithm starts off with putting every vertex of the given graph in its
own cluster, such that \(\{\{v\} \mid v \in V\}\).
Afterwards, in a maximum of \(\vert V \vert - 1\) iterations, the two clusters
that share a connection and whose amalgamation increases the \textit{modularity}
\(Q\) the most, respectively decreases it the least, are merged
\cite{clauset2004modularity}.

Eventually, this greedy algorithm will converge in a local or even global
optimum regarding the \textit{modularity} of the computed clustering
\cite{clauset2004modularity}.

\paragraph{Louvain \cite{blondel2008modularity}}
The \textit{Louvain} algorithm was devised in 2008 by Blondel et al. and is
named after their university \cite{blondel2008modularity}.
It is based on a similar premise to that of \textit{Clauset-Newman-Moore}.
The algorithm by Blondel et al. consists of two distinct phases.

Before beginning with the first phase, every vertex is placed in their own cluster.
The first phase then checks for each vertex with which adjacent vertex it can
merge clusters to reach a maximum increase in \textit{modularity}.
Compared to \textit{Clauset-Newman-Moore}, only if the \textit{modularity}
can be actually increased, the clusters are merged \cite{blondel2008modularity}.
All vertices are iterated over and checked repeatedly until a local optimum
is reached \cite{blondel2008modularity}.

As part of the second phase then, a new weighted graph is constructed with
the clusters detected in the first phase as the vertices.
The weighted edges between the new vertices are the sum of all vertices
the newly connected vertices are constructed out of.
Additionally, self-loops are attached to every vertex \cite{blondel2008modularity}.

Finally, the first phase is applied to the newly constructed graph again.
These two phases are then iteratively alternated until the increase in
\textit{modularity} ends or reaches a plateau \cite{blondel2008modularity}.
Hence, the \textit{Louvain} is a heuristic algorithm as well.

Fortunato et al. showed that \textit{modularity} optimization suffers
the problem of not being able to identify clusters of small sizes, the so called
\textit{resolution limit problem} \cite{fortunato2007resolution}.
The main improvements proposed by Blondel et al. specifially compared
to the algorithm by Clauset et al. are better runtime complexity, and a solution
to the \textit{resolution limit problem}, that \textit{Clauset-Newman-Moore}
faces, by leveraging the aforementioned self-loops \cite{blondel2008modularity}.

\paragraph{Infomap \cite{rosvall2008infomap}}
The \textit{Infomap} algorithm was devised in 2008 by Rosvall and Bergstrom
\cite{rosvall2008infomap}.
Its process bears a similarity to the \textit{Louvain} algorithm although
its core idea is entirely different. The similarity stems from the fact
that every vertex is also assigned to its own cluster in the beginning and
that cluster are then iteratively merged when this improves the metric that
the algorithm optimizes toward. However, compared to the \textit{Louvain}
algorithm, instead of \textit{modularity}, a \textit{map equation} is minimized.

Precisely, \textit{Infomap} represents the given graph as a random walk encoded
via Huffman coding \cite{huffman1952coding}.
Additionally, each cluster receives a prefix so that the codes for the vertices
can be reused in different clusters which decreases the required amount of bits
to represent the Huffman coding \cite{rosvall2008infomap}.

%TODO: One could describe a little more here, maybe?!

Finally, the phase that is reiterated over and over eventually tries to
change the clusters of the vertices such that the \textit{map equation}
representing the graph clustering is minimized \cite{rosvall2008infomap}.

\paragraph{Chinese Whispers \cite{biemann2006chinese}}
The \textit{Chinese Whispers} algorithm was released in 2005 by Biemann
\cite{biemann2006chinese}.
It mainly consists of an endless loop that is terminated once no better
clustering can be found anymore (i.e. a local or global optimum is reached)
or a predefined number of iterations has passed \cite{biemann2006chinese}.
At first, every vertex of the given graph is put into its own cluster.
Next, the vertices are iterated over in random order. Each vertex is moved
to the cluster that is shares the most edges with \cite{biemann2006chinese}.

Owing to the fact, that this algorithm is incorporating randomness,
it is non-deterministic. Moreover, it yields a flat clustering
(i.e. non-hierarchical) \cite{biemann2006chinese}.




\chapter{Related work} \label{chap:related}

The described problem of detecting latent boundaries within existing
application software for the purpose of decomposing it into microservices is
a sparse research domain, especially when compared to others
in computer science \cite{fritzsch2018monolith}.
Nevertheless, the existing research is categorized, rated and analyzed in the
following \textit{\nameref{subsect:literature-survey}}.

To add to this chapter, Fritzsch et al. conducted a complete literature study titled
\textit{From Monolith to Microservices: A Classification of Refactoring Approaches} (2019)
\cite{fritzsch2018monolith} which looked at research in the same domain as ours.
They conclude their research with:
\begin{displayquote}
\emph{"Our literature review has revealed a lack of systematic guidance on the
refactoring process for existing monolithic applications."}~\cite{fritzsch2018monolith}
\end{displayquote}
This underlines the relevance and importance of this area of research and the
notion that there is still room for improvement.



\section{Literature survey} \label{subsect:literature-survey}

\begin{table}[h!]
\def\arraystretch{1.55}
\begin{tabularx}{\textwidth}{|Y||Y|Y|Y|Y|Y|Y|Y|Y|Y|Y|}
 \hline
 \multicolumn{1}{|c||}{~} & \multicolumn{6}{c|}{Approaches} & \multicolumn{4}{c|}{Features}\\
 \hline\hline
 \rot{Citations~} & \rot{SE model analysis~} & \rot{Static analysis~} & \rot{Dynamic analysis~} & \rot{Data model analysis~} & \rot{VCS mining~} & \rot{External interface analysis~} &
 \rot{Evaluation~} & \rot{Evaluation by testing~} & \rot{Evaluation via metrics~} & \rot{(Available) Proof-of-concept~} \\
 \hline\hline
 \cite{gysel2016service}          & $\oplus$  & ~         & ~         & ~       & ~         & ~         & $\ominus$ & $\oplus$  & ~         & $\oplus$  \\\hline
 \cite{kruidenberg2018monoliths}  & $\odot$   & $\odot$   & $\odot$   & ~       & $\odot$   & ~         & $\odot$   & $\odot$   & $\ominus$ & $\oplus$  \\\hline
 \cite{mazlami2017extraction}     & ~         & $\odot$   & ~         & ~       & $\oplus$  & ~         & $\odot$   & $\oplus$  & $\odot$   & $\oplus$  \\\hline
 \cite{jin2018functionality}      & ~         & ~         & $\oplus$  & ~       & ~         & ~         & $\oplus$  & $\oplus$  & $\oplus$  & $\ominus$ \\\hline
 \cite{baresi2017microservices}   & ~         & ~         & ~         & ~       & ~         & $\oplus$  & $\oplus$  & $\oplus$  & $\odot$   & $\odot$   \\\hline
 \cite{chen2017monolith}          & $\oplus$  & ~         & ~         & ~       & ~         & ~         & $\odot$   & $\odot$   & ~         & $\ominus$ \\\hline
 \cite{escobar2016towards}        & ~         & $\oplus$  & ~         & ~       & ~         & ~         & $\odot$   & $\odot$   & $\odot$   & $\ominus$ \\\hline
 \cite{levcovitz2016towards}      & ~         & $\odot$   & ~         & $\odot$ & ~         & $\odot$   & $\ominus$ & $\ominus$ & ~         & $\ominus$ \\\hline
 \cite{knoche2018using}           & ~         & $\odot$   & ~         & ~       & ~         & ~         & $\ominus$ & $\ominus$ & ~         & $\ominus$ \\\hline
\end{tabularx}
\caption{Literature survey matrix}
\label{table:literature-survey}
\end{table}


The matrix (table \ref{table:literature-survey}) visualizes the
surveyed literature via classification.
Six general approaches as well as four features were extracted from the works.
The approaches were devised by studying the available literature on the subject.
In summary, other researchers were using one or more of these six approaches
to answer their research questions regarding microservice decomposition out of an existing monolith.
Thus, pure greenfield approaches were not targeted in the literature survey.\\
The signs used in the matrix are defined as follows:
\begin{itemize}[noitemsep]
    \item An empty cell depicts that the approach or feature of that column was not utilized
    by the work cited in that row.
    \item A circle with a dot ($\odot$) depicts that the approach or feature
    of that column was utilized by the work cited in that row without any outstanding execution
    or results in any direction or with results that were difficult to assess.
    \item A plus sign ($\oplus$) depicts that the approach or feature
    of that column was utilized by the work cited in that row in a successful way.
    \item A minus sign ($\ominus$) depicts that the approach or feature
    of that column was utilized by the work cited in that row in a subpar way.
\end{itemize}
Those evaluative signs were assigned via our own intuition of the execution and results
as well as the evaluation of the works in other analyzed literature.
The latter approach was especially possible due to
the existing work by Fritzsch et al. \cite{fritzsch2018monolith}.


\subsection*{Approaches}

In the following paragraphs, the approaches used as matrix columns are explained in more detail.
\paragraph{SE model analysis}
The term \textit{Software Engineering (SE) models} subsumes non-source code
artifacts out of the domain of SE.
For instance, this category includes use case models and entity relationship models (ERM).
\paragraph{Static analysis}
Static analysis covers any form of non-runtime analysis of the given code base.
This includes dependency graphs, class graphs, connection of source code via shared terms
(i.e. words in the identifier of classes that might tie them together) etc.
\paragraph{Dynamic analysis}
Dynamic analysis is about analyzing the existing monolithic software during its execution.
This covers profiling it, tracing data inside of the application as well as analyzing
access logs such as web traffic logs.
\paragraph{Data model analysis}
Owing to the fact that one pillar of the microservices architecture is about
separating data stores \cite{ms-microservices}, the approach to analyze
the data models (e.g. schemes) of existing database tables is, generally speaking,
an important one. This approach covers direct analysis of database tables
as well as the analysis of \textit{object relational mappers} (ORM)
if those were already in use in the existing application.
\paragraph{VCS mining}
The repositories of existing solutions are usually part of a
version control system (VCS) such as Git. Consequently, commits and merges can be analyzed
to e.g. determine the coupling of certain files, modules, or classes.
\paragraph{External interface analysis}
The external interfaces (i.e. the Web-APIs) of existing solutions can be analyzed to get
a sense of existing modularization and cohesion within the solution.


\subsection*{Features}

In the following paragraphs, the features used as matrix columns are explained in more detail.
\paragraph{Evaluation}
This column just depicts if the cited source of a row evaluated their work in any form.
\paragraph{Evaluation by testing}
Since this literature survey was focused on works that research assisted or semi-automated
microservice decomposition, theoretically, the works could be evaluated with real-world or
even artificial data such as large open source projects built with a monolithic architecture.
\paragraph{Evaluation via metrics}
Owing to the fact that the area of research of this project is still rather young and sparse
\cite{jin2018functionality}\cite{fritzsch2018monolith}, custom as well as renowned metrics
for the evaluation are a welcomed addition to the research in this area.
Additionally, evaluation via metrics improves the overall credibility of a work.
\paragraph{(Available) Proof-of-concept}
This feature is somewhat tied to the \textit{Evaluation by testing} one as oftentimes
a prototype (i.e. proof-of-concept) is built to actually test the designed
methodology or algorithm. Moreover, a positive or a negative execution respectively
was awarded if the prototype was publicly available (e.g. as open source software)
since it is otherwise impossible to assess.



\section{Topic relations} \label{subsect:topic-relations}

Following the approaches and features extracted in the previous section,
the methodologies of the works that are the closest to ours are observed with greater detail.

Gysel and K{\"o}lbener \cite{gysel2016service} were the first to release work in
this area of research that is in the same vein to ours in the sense that a
complete methodology is developed to convert a certain input to microservice
recommendations. They initially devised the idea to build a graph out of this
input that is then clustered using a graph clustering algorithm to create clusters
that reflect the microservice recommendations.
Future work in the domain \cite{mazlami2017extraction}\cite{kruidenberg2018monoliths}
together with ours is following this idea.
However, not every analyzed work uses this approach.
Therefore in this subsection, only works that do use it are analyzed in detail
as they relate closely in topic.


\subsection{Analyzed inputs}

Gysel and K{\"o}lbener used \textit{SE model analysis} as their only approach ---
a type of input that is unlikely to be available or
can be created with a lot of manual work, as they acknowledge
themselves in their original work \cite{gysel2016service}.
This fact decreases the actual usability of the devised methodology and leaves
room for improvement.

Kruidenberg \cite{kruidenberg2018monoliths} built on top of the work and
especially the implementation of Gysel and K{\"o}lbener by extending their
\textit{SE model analysis} input with \textit{Dynamic analysis} input.
Precisely, they model the input, that the implementation of Gysel and K{\"o}lbener
requires and generates out of \textit{SE models}, from data retrieved
out of \textit{Static analysis}, \textit{Dynamic analysis}, and \textit{VCS mining}.
Consequently, this approach mainly solves the problem that the original approach
had regarding the availability and respectively the ease of creation
of supported input formats.

Comparatively, Mazlami \cite{mazlami2017extraction} devised their completely
custom methodology. The foundation is still based on building and then clustering
a graph but they chose \textit{Static analysis}, and \textit{VCS mining} as
their approaches of choice. From within the domain of static program analysis,
semantically analyzing source code was devised and implemented as input together
with mining evolutionary data out of version control system logs.

Instead, we utilize three distinct input dimensions for constructing
a more precise weighted graph which can then be clustered.


\subsection{Graph clustering}

Regarding the graph clustering, Gysel and K{\"o}lbener \cite{gysel2016service}
only utilized two graph clustering algorithms both of which have major
shortcomings specific to this area of research.
The Girvan-Newman algorithm \cite{girvan2002community} requires the
amount of clusters beforehand which is not ideal for recommending
microservice candidates as the software engineers prospectively using
such a tool can at best estimate that amount.
Additionally, one advantage of automated microservice decomposition recommendations
is that the algorithm might give recommendations that a human engineer would
not have considered which also affects the granularity and therefore the
amount of services (i.e. clusters).
The Leung algorithm \cite{leung2009community} on the other hand has the
disadvantage that it is non-deterministic which might cause completely
different recommendations with every execution.

Consequently, Kruidenberg \cite{kruidenberg2018monoliths} uses the same
algorithms as his work is an extension of the one of Gysel and K{\"o}lbener.

Mazlami \cite{mazlami2017extraction} leveraged the notion of the
minimal spanning tree (MST) of the input graph to perform the clustering.
Thus, their methodology is not based on state-of-the-art graph clustering algorithms
but a custom approach instead. They implemented an algorithm
that iteratively calculates the MST and removes the edge with the lowest similarity
from the graph. However, this approach has the same problem as the
Girvan-Newman algorithm \cite{girvan2002community} that Gysel and K{\"o}lbener
utilized in the sense that it requires the desired amount of services beforehand.

Instead, we show how to leverage heuristic optimization of the clustering and
therefore the recommendations toward a numeric metric via a fitness function
while incorporating a large set of state-of-the-art graph clustering algorithms
to be able to compare them and assess which one performs the best for a given input.
Thus, this removes the problem of having to define the desired amount of microservices
a priori.


\subsection{Evaluative metrics}

Gysel and K{\"o}lbener \cite{gysel2016service} just evaluated their
initially set requirements on a custom scale of 1--3 together with a devised
questionnaire to assess the quality of the recommendations.
The questionnaire however follows no numeric metrics and just allows answers
in a boolean manner which makes it more subjective.

Mazlami \cite{mazlami2017extraction} devised a set of six metrics that measure
the output of their methodology. Although some of them do not have the notion
of if a higher value is better or worse (e.g. \textit{Average Contributors per Microservice}).

Kruidenberg \cite{kruidenberg2018monoliths} used a combination of metrics from
the works of Gysel and K{\"o}lbener \cite{gysel2016service} and
Mazlami \cite{mazlami2017extraction}.

Instead, we devise an extensive set of numeric metrics that offers
an objective and quantifiable view on the resulting microservice recommendations
to be able to evaluate not only our methodology but the recommendations them self as well.




\chapter{Mapping microservice requirements to software quality metrics} \label{chap:rationale}

As the focal point of our work is to construct a graph-theoretic representation
of the input monolithic application, a rationale how the measured dimensions of
the input map to the output is deemed necessary to corroborate our research.
Thus, said rationale was devised using



\section{Pillars of microservices}

\section{Selecting and extracting software quality metrics}




\chapter{Extracting coupling information from software}

\section{Dynamic coupling}

\subsection{Profiling-based approach}

\subsection{Instrumentation-based approach}

\section{Semantic coupling}

\subsection{Natural language processing on source code}

\section{Logical coupling}

\subsection{Mining version control system data}




\chapter{Representing software as weighted graphs}

\section{Building a weighted graph for each input dimension}

\section{Merging input dimensional graphs into combined one}




\chapter{Creating microservice recommendations} \label{chap:creating-service-recommendations}

\section{Clustering combined weighted graph}




\chapter{Calculating metrics on microservice recommendations}

\section{Input fidelity}

\section{General clustering quality}

\section{Coupling modularity}




\chapter{Visualizing microservice recommendations}

\section{Network-based view}

\section{Tree-based view}

\section{Metrics view}




\chapter{Implementation}




\chapter{Results}

\section{Experimental setup}




\chapter{Discussion} \label{chap:discussion}

\section{Threats to validity} \label{sect:threats-to-validity}

\section{Future work} \label{sect:future-work}




\chapter{Conclusion} \label{chap:conclusion}






% Reference lists
\newpage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures
\newpage
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables
\newpage
\addcontentsline{toc}{chapter}{Bibliography}
% Separate the sources with 'bibtopic'
\bibliographystyle{plain}
\begin{btSect}{references}
\section*{\huge{References}}
\btPrintCited
\end{btSect}
\begin{btSect}{online}
\section*{\huge{Online Sources}}
\btPrintCited
\end{btSect}

\end{document}
